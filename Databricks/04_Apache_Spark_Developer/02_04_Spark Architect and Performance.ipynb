{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec7423b9-8466-4323-aa81-f9cb53946ea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4. Spark Architect and Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9fa17c3-5ca7-49a0-bc62-3587b0f9ff9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Clusters\n",
    "- Example with a Cluster has 1 Driver, 6 Workers. Each Worker has 1 Executor and 2 Cores.\n",
    "  - Driver: brain of cluster which allocate tasks and data to worker nodes\n",
    "  - Worker: receives tasks and data, performs and return result to Deiver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db942025-317a-4a19-b58e-b907bbf5258c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76936d44-88c2-4930-b799-b595ddd78a78",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Narrow vs Wide Transformation"
    }
   },
   "source": [
    "### Narrow vs Wide Transformation\n",
    "- Narrow Transformation: 1-1 Partition\n",
    "  - select, filter, cast, union\n",
    "  - Start with 1 memory partition, do transformations and data stay within the **same** memory partition\n",
    "- Wide Transformation: Causes Shuffle/Exchange\n",
    "  - distinct, groupBy, sort, join\n",
    "  - Redistribute data and then create **new** memory partitions\n",
    "  - Redistibuting or re-partitioning data so the data is grouped differently across partitions\n",
    "    - Based on data size we may need to decrease/increase the number of Shuffle partitions via ```spark.sql.shuffle.patitions```\n",
    "\n",
    "![Narrow vs Wide Transformation](./images/Narrow_And_Wide_Transformation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc3f5e56-7b86-469f-b02a-8bbc3bc4011a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Process of Narrow Transformation\n",
    "Example: Filter color != \"brown\". We have 12-16 memory partitions of data\n",
    "- Step 1: Driver puts data files into 12-16 equal sized parition\n",
    "- Step 2: Driver allocates 12 partitions to 12 Cores, each Core gets 1 partition --> Opps we still have 4 partitions left\n",
    "- Step 3: 4 Cores finish early and return result to Driver. The other Cores are still processing\n",
    "- Step 4: Driver allocates 4 partitions for another iteration to the 4 Cores. The other Cores finish\n",
    "- Step 5: 4 Cores finish the 2nd iteration and return resul to Driver\n",
    "- Step 6: Driver collects the result and delivers to the client\n",
    "\n",
    "![Process of Narrow Transformation](./images/Narrow_Transformation_Example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4461b189-b10d-496d-95fb-32924e6999d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Process of Wide Transformation\n",
    "Example: Count total rows for each color. We have **19.5 MiB** data size, with 6 initial partitions\n",
    "- Stage 1: Local cCount\n",
    "  - Step 1: Driver allocates 6 partitions to 6 Cores, each Core gets 1 partition\n",
    "  - Step 2: 6 Cores finish early and **write** the result into Disk in dictionary key:value, so the file is only **568B**. For example:\n",
    "    - Core 2: Red:3, Blue:5, Yellow:7\n",
    "    - Core 3: Red:4, Blue:4, Yellow:8\n",
    "    - ...\n",
    "    - Core 12: Red:7, Blue:5, Yellow:5\n",
    "- Stage 2: Global Count\n",
    "  - Step 1: Driver allocates Core 7 to read the counts and do the \"Global Count\", then send the result\n",
    "  - Step 2: Core 7 **read** and sum the Local Counts and return the result to Driver\n",
    "  - Step 3: Driver collects the result and delivers to the client\n",
    "\n",
    "![Process of Wide Transformation](./images/Wide_Transformation_Example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "081c36e9-2d22-4e1f-b94e-3a84a299969c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Performance and Query Optimization\n",
    "There are 5 plans:\n",
    "\n",
    "**Input: Query, no matter programming language -->**\n",
    "1. Unresolved Logical Plan\n",
    "    - Drive reviews and confirms the **```schema correct?```**\n",
    "\n",
    "2. Analyzed Logical Plan\n",
    "    - Drive looks at **[Metadata Catalog] for ANALYSIS** and **```add data types```** to the columns\n",
    "\n",
    "3. Optimized Logical Plan\n",
    "    - Driver looks at **[Catalyst Catalog] for LOGICAL OPTIMIZATION** and check to apply number of rules-based (Filter, Join, etc.) to determine whether to **```move Filter before Join?```**\n",
    "\n",
    "4. Physical Plan\n",
    "    - Driver comes up with several ways (plans) to address the query **for PHYSICAL PLANNING** to determine **```which Join strategy to use, Data Skipping, Predicate Pushdown?```**\n",
    "\n",
    "5. Selected Physical Plan\n",
    "    - Driver puts ways in **[Cost Model] for WHOLE-STAGE CODE GENERATION** to see which way (plan) needs lowest cost = the best\n",
    "    - The plan would be **```Java bytecode and sent to Executors```**\n",
    "\n",
    "**--> Output: RDD, no matter dataframe, sql table or view**\n",
    "\n",
    "![Query Optimization](./images/Query_Optimization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad1edcfc-b585-45fb-b8b0-73ce313bc55d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Explain\n",
    "![Explain](./images/Explain.png)\n",
    "\n",
    "For each step, read from bottom to top\n",
    "\n",
    "**Step 1: Parsed Logical Plan**\n",
    "- It is shown as the script is wrote: Inner Join then Filter\n",
    "- Check the schema, e.g. lastname, firstname. dept... (no data types)\n",
    "\n",
    "**Step 2: Analyzed Logical Plan**\n",
    "- Add data type to each column of schema in step 1\n",
    "- Add CAST to ```dept``` to double as it is string\n",
    "\n",
    "It does not display all possible plans, but a optimized plan\n",
    "\n",
    "**Step 3: Optimized Logical Plan**\n",
    "- Move the Filter before Join, to get less rows to join\n",
    "\n",
    "**Step 4: Physical PLan**\n",
    "- Pick the best plan: Filter before Join and choose \"Sort Merge Join\" instead of \"Inner Join\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b853a12-6f04-44cc-930e-d68cb6ef033c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cost Based Optimization (CBO): Smaller Tables Join\n",
    "![Cost Based Optimization (CBO)](./images/CBO.png)\n",
    "\n",
    "- Enable configurations: CBO, join reorder\n",
    "- In script, (2) we join **```large```** table to **```medium```** table, then (1) we join **```small```** table\n",
    "- In CBO step, it chooses to:\n",
    "  - (1) get **```small```** table join with **```medium```** table then join with **```large```** table last\n",
    "  - Afterward, (2) it does shuffle/exchange\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fda882c7-3d65-49bd-bc8b-9d57ce32aee2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Adaptive Query Execution (AQE)\n",
    "![Adaptive Query Execution (AQE)](./images/AQE.png)\n",
    "- After RDD created, Spark will look at Statistics and shuffle count to see how big they are. Then, it will turn back to Analyzed Logical Plan to see whether it can fine-tune the number of shuffle\n",
    "\n",
    "\n",
    "![With And Without AQE](./images/With_Without_AQE.png)\n",
    "- **Without AQE:** we (1) start with 4 memory partitions and (2) end up with 200 memory partitions. **_But why does it need to shuffle 200 partitions for only 568B?_**, which mean 2B foreach partition --> too small\n",
    "- **With AQE:** we (1) start with 4 memory partitions and (2) end up with 1 partition, and much faster, even without parallism\n",
    "\n",
    "\n",
    "![Number of Jobs of AQE](./images/AQE_Number_Of_Jobs.png)\n",
    "- Number of jobs **increase** from 1 to 3 (job #3, #4, #5)\n",
    "  - Job #3 filters at WHERE clause to reduce size.\n",
    "  - Job #4 with AQE, needs 1 Shuffle\n",
    "- Hence, it **has cost overhead** with it, but it has benefit in long run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52f1ed58-598e-45a8-8b7c-9ec00ca0dff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Predicate Pushdown on RDDs\n",
    "- Predicate Pushdown is when the data source actively **limits the number** of rows returned to Spark reader vi SELECT/WHERE/FILTER\n",
    "- Predicate Pushdown filters the data in the database query,\n",
    "  - Reducing the number of entries retrieved from the source database and \n",
    "  - Improving query performance\n",
    "  - By default, the Spark Dataset API will automatically push down valid WHERE clauses to the database\n",
    "- **Cast** function cannot be Pushed down\n",
    "\n",
    "![Predicate Pushdown](./images/Predicate_Pushdown.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa928622-46c9-4552-9bf3-6ff9d087fa1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Caching - Best Practices\n",
    "- Don't cach unless you're sure a DataFrame will be **used multiple times**\n",
    "  - e.g. EDA (Exploratory Data Analysis), ML traning dataset\n",
    "- Omit unneeded columns to reduce the storage footprint\n",
    "- After calling `.cache()` which is **lazy transformation**, ensure all partitions are accessed with **Action**\n",
    "  - e.g. `count()` - put RDD into Cache\n",
    "- Manually evict cache when not needed\n",
    "  - e.g. `unpersist()` - remove RDD from Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69bfea2f-b6d8-40da-83ad-9d4f4d5ae675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Memory Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5afd407d-ae08-4e37-ab8a-5bcc46f91c46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Guidelines\n",
    "- Error on the side of **too many small** than to few large Memory Partitions. If so large memory, then the core does not have enough memory, leading 2 possible consequences:\n",
    "  - Spill to disk, waiting for more RAM, then bring it back\n",
    "  - OOM: out of memory error\n",
    "- Sweet spot initial size: **128MB and 1GB**\n",
    "- Calculate the size of Shuffle partitions by dividing Shuffle stage input (4TB) by the target partition size (200MB).\n",
    "  - e.g. 4TB / 200MB = 20,000 Shuffle Partition count\n",
    "  - By default, it is 200 `spark.conf.get(\"spark.sql.shuffle.partitions\")`\n",
    "- Can manually set number of Shuffle Partitions on case-by-case basis\n",
    "  - `spark.conf.set(\"spark.sql.shuffle.partitions\", \"20000\")`\n",
    "  - This setting is Local for **1 session** only.\n",
    "\n",
    "![200_Default_Partitions](./images/200_Default_Partitions.png)\n",
    "- Example: \n",
    "  - It starts with 8 partitions, then spawns 200 shuffle partitions\n",
    "  - But there are only 42KB (~no thing) to write\n",
    "  - Even some of tasks which means memory partitions reside has 0 bytes (blank)\n",
    "  - It looks like AEQ turned off --> turn it on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48018e1d-0d5a-40fe-bf51-e6956ca027b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cores in Cluster\n",
    "- Initially, the Driver determines the number of Memory Partitions and its size. It decides based on:\n",
    "  - Number of Cores in Cluster. \n",
    "  - More Cores, more Patitions.\n",
    "- Get to number of Cores by 2 ways:\n",
    "  - `sc.defaultParallelism` or `spark.sparkContext.defaultParallelism`\n",
    "  - Spark UI -> Cores\n",
    "  \n",
    "  ![Spark UI](./images/Spark_UI_Cores.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c484fd21-1e26-4b03-a7b3-45c326b2aedf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### No. of Memory Partition for DataFrame\n",
    "- If Memory Partitions are sized too large (> 1GB), we can manually change in No. of Partitions (to higher number) to get them into a more reasonable size rang (**128MB** to **1GB**)\n",
    "- AQE can resolve some Partitions issues\n",
    "  - e.g. for small dataset, AQE won't create default 200 Shuffle Partitions, but rather a far lower number\n",
    "- We need to **convert DataFrame into RDD** to get number of Partitions used for the DataFrame\n",
    "  - `df.rdd.getNumPartitions()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a475e43b-2dce-4310-987b-57880042fb56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Re-Partition a DataFrame\n",
    "There are 2 ways:\n",
    "1. **`coalesce(int)`:**\n",
    "    - Returns new DF with exactly N partitions when N < current No. of Partitions\n",
    "    - **Narrow** transformation\n",
    "    - Pros:\n",
    "        - Retain sort order\n",
    "        - No shuffle\n",
    "    - Cons:\n",
    "        - Only decrease No. of Partitions\n",
    "        - Unevenly balanced partition sizes\n",
    "\n",
    "2. **`repartition(int, [col])`:**\n",
    "    - Return new DF with exactly N partitions\n",
    "    - **Wide** transformation\n",
    "    - Pros:\n",
    "        - Evenly balanced partition sizes\n",
    "        - Both increase/decrease No. of Partition\n",
    "    - Cons:\n",
    "        - Not retain sort order\n",
    "        - Require Shuffle\n",
    "\n",
    "**Notes:**\n",
    "  - More No. of Partition, less size\n",
    "  - Less No. of Paritions, more size"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_04_Spark Architect and Performance",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
