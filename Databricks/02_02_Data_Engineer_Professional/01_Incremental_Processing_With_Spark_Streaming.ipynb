{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f27760e9-8cee-4b2c-853b-d02d780c634c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Streaming Data Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f745d570-2cd1-4410-91a4-f3a3a0e0ae7c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## What is streaming data?\n",
    "- Traditional **batch-oriented** data processing: is **one-off** and **bounded**\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Batch_Processing.png\" alt=\"Batch_Processing\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "- **Streaming** processing: is **continuous** and **unbounded**\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Streaming_Processing.png\" alt=\"Streaming_Processing\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "118d4e86-dedc-407b-8927-58583b82df5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Bounded vs. Unbounded Dataset\n",
    "**1. Bounded Data:**\n",
    "- Has a **finite** and **unchanging** structure at the time of processing\n",
    "- The order is **static**\n",
    "- Analogy: Vehicles in a parking lot\n",
    "\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Anology_Bounded_Data.png\" alt=\"Anology_Bounded_Data\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "**2. Unbounded Data:**\n",
    "- Has an **infinite** and **continuousely changing** structure at the time of processing\n",
    "- The order **not always sequential**\n",
    "- Analogy: Vehicles on a highway\n",
    "\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Anology_Unbounded_Data.png\" alt=\"Anology_Unbounded_Data\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "599ce620-c120-4d65-8186-1e620b7da33b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Batch vs. Streaming Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96fd6f0e-1b40-4d70-8c98-5d02359951bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Batch Processing\n",
    "- Refers to processing & analysis of **bounded** datasets\n",
    "  - e.g. size is well known, we can count the number of elements\n",
    "- **Loose** data latency requirement\n",
    "  - e.g. day old, week old\n",
    "- **Traditional ETL** from transactional systems into analytical systems\n",
    "\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Batch_Example.png\" alt=\"Batch_Example\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0034aa36-c1b5-47b9-aae8-d0198b2c1c26",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Streaming Processing\n",
    "- Datasets are **continuous** and **unbounded**\n",
    "  - Data is constantly arriving and must be processed as long as there is new data --> **Micro-batch (or 1-by-1)**\n",
    "- **Low-latency** use cases\n",
    "  - e.g. real-tiome or new real-time\n",
    "- Provide fast, actionable insights\n",
    "  - e.g. Quality-of-Service, Device Monitoring, Recommendations)\n",
    "\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Streaming_Example.png\" alt=\"Streaming_Example\" style=\"border: 2px solid black; border-radius: 10px;\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0543bfb1-9ae1-4fd8-9959-646dd4b5b235",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Similarities\n",
    "- Both have data transformation\n",
    "- Output of streaming job is often queried in batch jobs\n",
    "- Stream processing oftern inlcude batch processing (micro-batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5459681-993c-45d3-9092-8bcd8876cc14",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Differences:\n",
    "|How to process in one run | Batch | Streaming |\n",
    "|-|-|-|\n",
    "|**Bounded dataset** | Big batch | Row by row / mini-batch|\n",
    "|**Unbounded dataset** | N/A (multi runs) | Row by row / mini-batch|\n",
    "|**Query computation** | Only once | Multiple |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2761040-ed0b-4778-b005-33103fdc85db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Instroduction to Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc26feb7-aaed-4028-9d0a-a5dae0ad3bb8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## What is Structured Streaming\n",
    "- A scalable, fault-tolerant **stream processing framework** built on Spark SQL engine\n",
    "- Uses **existing structured APIs** (DataFrames, SQL Engine) and provides similar API as batch processing API\n",
    "- Includes **stream specific features**, end-to-end, exactl-once-processing, fault-tolerance, et.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7c6c42c-5f23-4fd2-9f59-695abb8f0ec7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## How Structured Streaming Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30b7b30f-c23a-42e9-ab29-b6dc3525c95c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Incremental Updates - Data stream as an unbounded table\n",
    "- Streaming data is usually coming in very fast\n",
    "- The magic behind Spark Structure Streaming: Processing infinite data as an incremental table updates\n",
    "\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Incremental_Updates.png\" alt=\"Incremental_Updates\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bb92ec9-a998-4c5a-9562-82bf9c8136c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Micro-Batch Processing\n",
    "- **Micro-batch Execution:** **_accumulate_** **small batches** of data and process each batch in **parallel**\n",
    "- Continuous Execution (EXPERIMNENTAL): continuously **_listen_** for new data and process them **individually**\n",
    "\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Micro_Batch_Processing.png\" alt=\"Micro_Batch_Processing\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59960048-5019-40a4-8600-c864c86c1fef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Execution mode\n",
    "1. An **`input table`** is defined by configuring a **streaming read** against **source**\n",
    "2. A **`query`** is defined against the **`input table`**\n",
    "3. This logical query on the input table generates the **`results table`**\n",
    "4. The output of a streaming pipeline will persist updates to the **`results table`** by writing to an external **sink**\n",
    "5. New rows are appended to the **`input table`** for each trigger interval\n",
    "\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Programming_Model_For_Structured_Streaming.png\" alt=\"Programming_Model_For_Structured_Streaming\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6391e5b5-fed1-4ff9-929a-4e29c18fc57d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Anatomy of a Streaming Query\n",
    "- Core concepts:\n",
    "  - Input sources\n",
    "  - Sinks\n",
    "  - Transformations & actions\n",
    "  - Triggers\n",
    "\n",
    "- Example:\n",
    "  - Read JSON data from Kafka\n",
    "  - Parse nested JSON\n",
    "  - Store in structured Delta Lake table\n",
    "\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/JSON_Example.png\" alt=\"JSON_Example\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e09f6462-99c2-4fbf-b0b0-08a45ce51b21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Source\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Core_Concept_Source.png\" alt=\"Core_Concept_Source\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "```\n",
    "spark.readStream.format(\"kafka\")\n",
    " .option(\"kafka.bootstrap.servers\",...)\n",
    " .option(\"subscribe\", \"topic\")\n",
    " .load()\n",
    "```\n",
    "- Specify where to read data from\n",
    "- OS Spark supports Kafka and file sources\n",
    "- Databricks runtimes include connector libraries supporting Delta, Event Hubs, and Kinesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a240087b-17be-4cbd-a82b-06cd689995fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Transformation\n",
    "```\n",
    "spark.readStream.format(\"kafka\")\n",
    " .option(\"kafka.bootstrap.servers\",...)\n",
    " .option(\"subscribe\", \"topic\")\n",
    " .load()\n",
    " .selectExpr(\"cast (value as string) as json\")\n",
    " .select(from_json(\"json\", schema).as(\"data\"))\n",
    "```\n",
    "- 100s of built-in, optimized SQL functions like from_json\n",
    "- In this example, cast bytes from Kafka records to a string, parse it as JSON, and generate nested columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8601d2b5-fb5b-424d-b67a-db60d1a5cad6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Aggregations, Time Windows, Watermarks"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Incremental_Processing_With_Spark_Streaming",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
