{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f27760e9-8cee-4b2c-853b-d02d780c634c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Streaming Data Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f745d570-2cd1-4410-91a4-f3a3a0e0ae7c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## What is streaming data?\n",
    "- Traditional **batch-oriented** data processing: is **one-off** and **bounded**\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Batch_Processing.png\" alt=\"Batch_Processing\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "- **Streaming** processing: is **continuous** and **unbounded**\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Streaming_Processing.png\" alt=\"Streaming_Processing\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "118d4e86-dedc-407b-8927-58583b82df5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Bounded vs. Unbounded Dataset\n",
    "**1. Bounded Data:**\n",
    "- Has a **finite** and **unchanging** structure at the time of processing\n",
    "- The order is **static**\n",
    "- Analogy: Vehicles in a parking lot\n",
    "\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Anology_Bounded_Data.png\" alt=\"Anology_Bounded_Data\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "**2. Unbounded Data:**\n",
    "- Has an **infinite** and **continuousely changing** structure at the time of processing\n",
    "- The order **not always sequential**\n",
    "- Analogy: Vehicles on a highway\n",
    "\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Anology_Unbounded_Data.png\" alt=\"Anology_Unbounded_Data\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "599ce620-c120-4d65-8186-1e620b7da33b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Batch vs. Streaming Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96fd6f0e-1b40-4d70-8c98-5d02359951bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Batch Processing\n",
    "- Refers to processing & analysis of **bounded** datasets\n",
    "  - e.g. size is well known, we can count the number of elements\n",
    "- **Loose** data latency requirement\n",
    "  - e.g. day old, week old\n",
    "- **Traditional ETL** from transactional systems into analytical systems\n",
    "\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Batch_Example.png\" alt=\"Batch_Example\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0034aa36-c1b5-47b9-aae8-d0198b2c1c26",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Streaming Processing\n",
    "- Datasets are **continuous** and **unbounded**\n",
    "  - Data is constantly arriving and must be processed as long as there is new data --> **Micro-batch (or 1-by-1)**\n",
    "- **Low-latency** use cases\n",
    "  - e.g. real-tiome or new real-time\n",
    "- Provide fast, actionable insights\n",
    "  - e.g. Quality-of-Service, Device Monitoring, Recommendations)\n",
    "\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Streaming_Example.png\" alt=\"Streaming_Example\" style=\"border: 2px solid black; border-radius: 10px;\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0543bfb1-9ae1-4fd8-9959-646dd4b5b235",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Similarities\n",
    "- Both have data transformation\n",
    "- Output of streaming job is often queried in batch jobs\n",
    "- Stream processing oftern inlcude batch processing (micro-batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5459681-993c-45d3-9092-8bcd8876cc14",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Differences:\n",
    "|How to process in one run | Batch | Streaming |\n",
    "|-|-|-|\n",
    "|**Bounded dataset** | Big batch | Row by row / mini-batch|\n",
    "|**Unbounded dataset** | N/A (multi runs) | Row by row / mini-batch|\n",
    "|**Query computation** | Only once | Multiple |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2761040-ed0b-4778-b005-33103fdc85db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Instroduction to Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc26feb7-aaed-4028-9d0a-a5dae0ad3bb8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## What is Structured Streaming\n",
    "- A scalable, fault-tolerant **stream processing framework** built on Spark SQL engine\n",
    "- Uses **existing structured APIs** (DataFrames, SQL Engine) and provides similar API as batch processing API\n",
    "- Includes **stream specific features**, end-to-end, exactl-once-processing, fault-tolerance, et.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7c6c42c-5f23-4fd2-9f59-695abb8f0ec7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## How Structured Streaming Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30b7b30f-c23a-42e9-ab29-b6dc3525c95c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Incremental Updates - Data stream as an unbounded table\n",
    "- Streaming data is usually coming in very fast\n",
    "- The magic behind Spark Structure Streaming: Processing infinite data as an incremental table updates\n",
    "\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Incremental_Updates.png\" alt=\"Incremental_Updates\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bb92ec9-a998-4c5a-9562-82bf9c8136c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Continuous Execution vs. Micro-Batch\n",
    "- **Continuous Execution (EXPERIMNENTAL):** continuously **_listen_** for new data and process them **individually** --> lower throughput + overhead resource\n",
    "- **Micro-batch Execution:** **_accumulate_** **small batches** of data and process each batch in **parallel** --> higer throughput + fewer nodes\n",
    "\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Micro_Batch_Processing.png\" alt=\"Micro_Batch_Processing\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59960048-5019-40a4-8600-c864c86c1fef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Execution mode\n",
    "1. An **`input table`** is defined by configuring a **streaming read** against **source**\n",
    "2. A **`query`** is defined against the **`input table`**\n",
    "3. This logical query on the input table generates the **`results table`**\n",
    "4. The output of a streaming pipeline will persist updates to the **`results table`** by writing to an external **sink**\n",
    "5. New rows are appended to the **`input table`** for each trigger interval\n",
    "\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Programming_Model_For_Structured_Streaming.png\" alt=\"Programming_Model_For_Structured_Streaming\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6391e5b5-fed1-4ff9-929a-4e29c18fc57d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Anatomy of a Streaming Query\n",
    "- Core concepts:\n",
    "  - Input sources\n",
    "  - Sinks\n",
    "  - Transformations & actions\n",
    "  - Triggers\n",
    "\n",
    "- Example:\n",
    "  - Read JSON data from Kafka\n",
    "  - Parse nested JSON\n",
    "  - Store in structured Delta Lake table\n",
    "\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/JSON_Example.png\" alt=\"JSON_Example\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e09f6462-99c2-4fbf-b0b0-08a45ce51b21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Source\n",
    "**Source:**\n",
    "- Specify where to read data from\n",
    "- OS Spark supports Kafka and file sources\n",
    "- Databricks runtimes include connector libraries supporting Delta, Event Hubs, and Kinesis\n",
    "\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Core_Concept_Source.png\" alt=\"Core_Concept_Source\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "1. Instantiate Spark object -> read data as a stream with a format specification (in this example = kafka)\n",
    "2. Add option to run from kafka.bootstrap.servers\n",
    "3. Add option to subcribe to a topic.\n",
    "    - This can include multiple sources of different types\n",
    "4. Call load() function to return a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aab8782d-d3f5-4493-a26b-1cb80a67ca78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.readStream.format(\"kafka\")\n",
    " .option(\"kafka.bootstrap.servers\",...)\n",
    " .option(\"subscribe\", \"topic\")\n",
    " .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a240087b-17be-4cbd-a82b-06cd689995fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Transformation\n",
    "**Transformation:** add transformation of data on the fly\n",
    "\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Core_Concept_Transformation.png\" alt=\"Core_Concept_Transformation\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "1. Cast bytes from Kafka records to a string, parse it as  JSON and generate nested columns\n",
    "    - There are 100s of built-in optimized SQL functions from JSON, or UDF like lambdas function, map or flat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be3d8390-ec71-4c4b-a998-5dbe12350dce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.readStream.format(\"kafka\")\n",
    " .option(\"kafka.bootstrap.servers\",...)\n",
    " .option(\"subscribe\", \"topic\")\n",
    " .load()\n",
    " .selectExpr(\"cast (value as string) as json\")\n",
    " .select(from_json(\"json\", schema).as(\"data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d8d98c9-6073-4858-8fcf-11c89fd2c28b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Sink\n",
    "**Sink:** add storage to store data\n",
    "\n",
    "Built-in Spark supports:\n",
    "- Files and Kafka for production\n",
    "- Console and memory for development and debugging\n",
    "- `foreachBatch` to execute arbitrary code with the output data\n",
    "\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Core_Concept_Sink.png\" alt=\"Core_Concept_Sink\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "1. Write transformed output to external storage systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "646bc880-04d8-43ca-a79d-62220658b4f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.readStream.format(\"kafka\")\n",
    " .option(\"kafka.bootstrap.servers\",...)\n",
    " .option(\"subscribe\", \"topic\")\n",
    " .load()\n",
    " .selectExpr(\"cast (value as string) as json\")\n",
    " .select(from_json(\"json\", schema).as(\"data\"))\n",
    " .writeStream\n",
    " .format(\"delta\")\n",
    " .option(\"path\", \"/deltaTable/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb31e6ef-551a-4bc7-9448-ccbfd91ac87e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Checkpoint\n",
    "**Checkpoint location:** For tracking the progress of the query\n",
    "\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Core_Concept_Checkpoint.png\" alt=\"Core_Concept_Checkpoint\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59a5d0bb-58f2-42d3-b4d4-a4580f0bfed2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.readStream.format(\"kafka\")\n",
    " .option(\"kafka.bootstrap.servers\",...)\n",
    " .option(\"subscribe\", \"topic\")\n",
    " .load()\n",
    " .selectExpr(\"cast (value as string) as json\")\n",
    " .select(from_json(\"json\", schema).as(\"data\"))\n",
    " .writeStream\n",
    " .format(\"delta\")\n",
    " .option(\"path\", \"/deltaTable/\")\n",
    " .trigger(\"1 minute\")\n",
    " .option(\"checkpointLocation\", \"…\")\n",
    " .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc2acf7a-96c7-4ba1-ae75-dbedd3c6a61d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Trigger\n",
    "**Trigger:** Defines how frequently the input table is checked for new data\n",
    "- Each time a trigger fires, Sparks check for new data and updates the results\n",
    "\n",
    "| Type | Script | Description |\n",
    "|-|-|-|\n",
    "| Fixed interval micro batches | `.trigger(processingTime = \"2 minutes\")` | Micro-batch processing kicked off at the **user-specified interval** |\n",
    "| Triggered One-time micro batch | `.trigger(once=True)` | Process all of the available data as a **single `micro-batch`** and then automatically stop the query |\n",
    "| Triggered One-time micro batches | `.trigger(availableNow=True)` | Process all of the available data as **multiple `micro-batches`** and then automatically stop the query |\n",
    "| Continuous Processing | `.trigger(continuous= \"2 seconds\")` | Long-running tasks that **continuously** read, process, and write data as soon events are available, with checkpoints at the specified frequency |\n",
    "| Default  |  | Databricks: 500ms fixed interval OS Apache Spark: Process each microbatch as soon as the previous has been processed |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d0a65d6-b251-4ff2-9dad-0d15ab9c495c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Output Mode\n",
    "\n",
    "**Output Mode:**\n",
    "- Defines how the data is written to the sink\n",
    "- Equivalent to **_\"save\"_** mode on static DataFrames\n",
    "\n",
    "| Type | Description |\n",
    "|-|-|\n",
    "| Complete  | The entire updated Result Table is written to the sink. The individual sink implementation decides how to handle writing the entire table |\n",
    "| Append  | Only the new rows appended to the Result Table since the last trigger are written to the sink |\n",
    "| Update  | Only new rows and the rows in the Result Table that were updated since the last trigger will be outputted to the sink |\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> The output modes supported depends on the type of transformations and sinks used by the streaming query. Refer to the the [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) for details.\n",
    "\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Core_Concept_Output.png\" alt=\"Core_Concept_Output\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "509e195f-59ba-4a10-ae4b-36d96d90caad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.readStream.format(\"kafka\")\n",
    ".option(\"kafka.bootstrap.servers\",...)\n",
    ".option(\"subscribe\", \"topic\")\n",
    ".load()\n",
    ".selectExpr(\"cast (value as string) as json\")\n",
    ".select(from_json(\"json\", schema).as(\"data\"))\n",
    ".writeStream\n",
    ".format(\"delta\")\n",
    ".option(\"path\", \"/deltaTable/\")\n",
    ".trigger(\"1 minute\")\n",
    ".option(\"checkpointLocation\", \"…\")\n",
    ".outputMode(\"complete\")\n",
    ".start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5786cd78-2e83-4933-851b-3fdf4b7ad2fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Benefits of Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8efddc03-b8f2-4575-8920-5112640c22c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Unification\n",
    "- **Same API** is used for batch and stream processing.\n",
    "- Supports Python, SQL or Spark’s other supported languages.\n",
    "- Spark’s built-in libraries can be called in a streaming context, including ML libraries.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> Most operations on a streaming DataFrame are identical to a static DataFrame. There are some exceptions to this, for example, **_sorting_** is not supported with streaming data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cd77830-ba42-40de-b64e-c05012aa3b87",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Fault Tolerance\n",
    "- Structured Streaming ensures end-to-end exactly-once fault-tolerance guarantees through **`checkpointing`**.\n",
    "- In case of failures; the streaming engine attempts to **_restart_** and/or **_reprocess_** the data.\n",
    "- This approach requires;\n",
    "  - **Replayable streaming** source such as cloud-based object storage and pub/sub services.\n",
    "  - **Idempotent sinks** - multiple writes of the same data (as identified by the offset) **_do not result in duplicates_** being written to the sink."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "874d0a38-13ae-40d2-939d-6dca7e8e9b08",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Handle Out-of-Order Data\n",
    "- Supports **event-time-window-based** aggregation queries\n",
    "- Supports **watermarking** which allows users to the threshold of late data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0127c0a8-fa68-489c-adf1-de9b54a7769b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Structured Streaming with Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19320543-68cb-4f43-8a90-b2b431cb52e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Delta Lake Benefits\n",
    "\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Detal_Lake_Benefits.png\" alt=\"Detal_Lake_Benefits\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c68f3faf-dd2e-4dc6-910a-6c33caf75046",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Streaming from Delta Lake\n",
    "- Each committed version represents **new data** to stream. Delta Lake **transactions logs** identify the version’s new data files\n",
    "- Structured Streaming assumes **append-only** sources. Any non-append changes to a Delta table causes queries streaming from that table to **throw exceptions**.\n",
    "  - Set `delta.appendOnly = true` to prevent non-append modifications to a table.\n",
    "  - Use Delta Lake [change data feed](https://docs.databricks.com/en/delta/delta-change-data-feed.html) to propagate arbitrary change events to downstream consumers\n",
    "- You can [limit the input rate](https://docs.databricks.com/en/structured-streaming/delta-lake.html#limit-input-rate) for micro-batches by setting `DataStreamReader` options:\n",
    "  - `maxFilesPerTrigger`: Maximum files read per micro-batch (default 1,000)\n",
    "  - `maxBytesPerTrigger`: Soft limit to amount of data read per micro-batch (no default)\n",
    "  - **Note:** Delta Live Tables pipelines auto-tune options for rate limiting, so you **_`should avoid`_** setting these options explicitly for your pipelines.\n",
    "- Each micro-batch written to the Delta table is committed as a **new version**.\n",
    "- Delta Lake supports both **`append`** and **`complete`** output modes.\n",
    "  - Append is most common.\n",
    "  - Complete replaces the entire table with each micro-batch. It can be used for streaming queries that perform arbitrary aggregations on streaming data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8601d2b5-fb5b-424d-b67a-db60d1a5cad6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Aggregations, Time Windows, Watermarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6af0c2ca-57a1-4345-b86e-5cff3de250ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Types of Streaming Processing\n",
    "|Stateless| Stateful |\n",
    "|-|-|\n",
    "| Typically **trivial transformations**. The way records are handled **_do not depend_** on previously seen records. | Previously seen records **_can influence_** new records |\n",
    "| Example: Data Ingest (map-only), simple dimensional joins | Example: Aggregations over time, Fraud/Anomaly Detection |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a790678f-7b98-498b-9052-c20c321cf134",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Stream Aggregations\n",
    "- Continuous applications often require **near real-time** decisions on **real-time**, aggregated statistics\n",
    "  - Examples: Aggregating errors from IoT devices, behavior analysis on instant messages via hashtags\n",
    "- In the case of streams, you generally don't want to run aggregations **_over the entire dataset_**. Why;\n",
    "  - There conceptually is **no end** to the flow of data, data is **continuous**\n",
    "  - The size of the dataset grows in **perpetuity**; will eventually **_run out of resources_**\n",
    "- **Solution:** Instead of aggregating over the entire dataset, we can aggregate over data **_grouped by_ `windows`** of time (say, every 5 minutes). This is referred to as **`windowing`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e283237-a466-4175-a338-f61439d1a8e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Time Based Windows\n",
    "| Tumbling Window | Sliding Window |\n",
    "|-|-|\n",
    "|**No window overlap** | **Windows overlap** |\n",
    "| Any given event gets aggregated into **only one** window group (e.g. 1:00–2:00 am, 2:00–3:00 am, 3:00-4:00 am, …) |Any given event gets aggregated into **multiple window** groups (e.g. 1:00-2:00 am, 1:30–2:30 am, 2:00–3:00 am, …) |\n",
    "\n",
    "**Sliding Window Example:**\n",
    "\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Sliding_Window_Example.png\" alt=\"Sliding_Window_Example\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "165462ca-32f7-43b0-a3aa-0d617390b509",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Reasoning About Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be29efc8-ac66-482e-a483-4d4777bf66a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Event Time vs. Processing Time\n",
    "\n",
    "- **Event Time:** time at which the **event** (record in the data) actually occurred.\n",
    "- **Processing time:** time at which a **record** is actually processed.\n",
    "- Important in every use case processing unbounded data in whatever order (otherwise no guarantee on correctness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e98d94be-9e60-4aca-8c20-f6a3059c7041",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Time Doamin Skew\n",
    "**When batch processing:**\n",
    "- **`Processing time`** per definition much later (e.g. an hour or day) than **_`event time`_**\n",
    "- Data assumed to be complete (or settle for incompleteness)\n",
    "\n",
    "**When stream processing:**\n",
    "- **`Processing time`** >= **_`event time`_** but often close (e.g. seconds, minutes)\n",
    "- Challenge when **`processing time`** >>> **_`event time`_** (`**late data**`): not able to conclude anything easily, how long to wait for the data to be complete?\n",
    "\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Time_Domain_Skew.png\" alt=\"Time_Domain_Skew\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bf533ec-2024-40fd-91b9-5710e2e3d17c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Watermarking\n",
    "**Watermark:** **_Handle_** **late** data and **_limit_** how long to **`remember`** old data\n",
    "- Analogy: Highway **minimum speed** limit\n",
    "<img src=\"./images/01_Incremental_Processing_With_Spark_Streaming/Watermarking.png\" alt=\"Watermarking\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Incremental_Processing_With_Spark_Streaming",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
