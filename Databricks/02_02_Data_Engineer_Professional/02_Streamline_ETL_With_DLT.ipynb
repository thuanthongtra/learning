{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f27760e9-8cee-4b2c-853b-d02d780c634c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data Ingestion Patterns\n",
    "**Limitations at Data Ingestion Stage:**\n",
    "- Streaming sources like Kinesis, Kafka and EventHubs only retain data for a **limited** amount of time\n",
    "- **Need for retention** - full history of data\n",
    "  - Reprocessing raw data\n",
    "  - Perform GDPR and compliance tasks\n",
    "  - Recover data\n",
    "- Need for a simple, **maintainable and scalable** architecture\n",
    "- Keeping full history in the streaming source is **expensive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c996dd5-8c99-42bc-b23a-d76e601a6c6c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Pattern 1: Use Delta for **Infinite Retention**\n",
    "**Delta:**\n",
    "- Provides **cheap**, **elastic** and **governable** storage for transient sources\n",
    "- It allows to **_stream_**, **_store_** and **_track_** data in datalake upfront\n",
    "\n",
    "<img src=\"./images/02_Streamline_ETL_With_DLT/Pattern_1.png\" alt=\"Pattern_1\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "**1. Input:** we can connect any of these input types into streaming system\n",
    "- cloud_file: confige Auto Loader to detect files coming in object storage\n",
    "\n",
    "**2. Transformation:**\n",
    "- Need to have **minimized transformation** to have **raw/original** form\n",
    "\n",
    "**3. Store Data:**\n",
    "- DLT allows to **_recompute_**, **_debug_** codes, **_delete_** and **_propagate_** changes in pipeline\n",
    "- In Bronze layer, `pipeline.reset.allowed=false` ensures downstream computations can be fully refreshed without losing data\n",
    "- Best pratice: \n",
    "    - **`streaming tables`** for **Bronze**\n",
    "    - **_live tables_** for **Silver** and **Gold**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eee2f070-2ff9-4b25-845e-8edd8f788cdd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Pattern 2: Up-to-date **Replica with CDC**\n",
    "Maintain an **up-to-date replica** of a table stored elsewhere\n",
    "- Use **Change Data Capture (CDC)** from RDMS and create replica as Delta\n",
    "- A variety of 3rd party tools can provide a streaming change feed\n",
    "- DLT simplifies with `APPLY CHANGES INTO`\n",
    "\n",
    "<img src=\"./images/02_Streamline_ETL_With_DLT/Pattern_2.png\" alt=\"Pattern_2\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "113bd883-2eb0-4f31-a9c8-cbb7464d95e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Pattern 3: Multiplex Ingestion\n",
    "**1. Simplex vs. Multiplex:**\n",
    "\n",
    "<img src=\"./images/02_Streamline_ETL_With_DLT/Simplex_Vs_Multiplex.png\" alt=\"Simplex_Vs_Multiplex\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "- **Simplex:** **every single** source is being streamed into **its own** Delta Lake table\n",
    "- **Multiplex:** \n",
    "  - **all** of sources are streamed into the **same** table and \n",
    "  - each source can be differentitated by **_adding_ a column** specifying source of the dataset\n",
    "\n",
    "**2. Anti-Pattern:** **_`Don't use`_** **Kafka** directly to **Bronze Table**:\n",
    "- Data retention **limited** by Kafka; **expensive** to keep full history\n",
    "- All processing happens on ingest\n",
    "- If stream gets too far behind, data is **lost**\n",
    "- Cannot recover data (no history to replay)\n",
    "\n",
    "<img src=\"./images/02_Streamline_ETL_With_DLT/Anti_Pattern_3.png\" alt=\"Anti_Pattern_3\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "**3. Bronze + Silver approach:**\n",
    "\n",
    "<img src=\"./images/02_Streamline_ETL_With_DLT/Pattern_3.png\" alt=\"Pattern_3\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "1. Ingest data into **Bronze table**\n",
    "2. **_Read_** Bronze table -> **_separate_** data by category -> **_apply_** a schema and some transformations -> **_write_** to **Silver Delta table** = multiplex ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a3c44fa-89bf-408a-b619-f6e1129bd120",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data Quality Enforcement Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d2e9267-055a-40f9-9ed1-27be95349b5f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Silver Layer for Quality Enforcement\n",
    "**Silver Layer Objectives:**\n",
    "- **_Validate_** data quality and schema\n",
    "- **_Enrich_** and **_transform_** data\n",
    "- **_Optimize_** data layout and storage for downstream queries\n",
    "- **_Provide_** single source of truth for analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa3efd84-b8fc-4e18-8362-eceaf4b33424",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Schema Enforcement & Evolution\n",
    "**1. Enforcement:** **_prevents_ bad records** from entering table\n",
    "- E.g. Mismatch in type or field name\n",
    "\n",
    "**2. Evolution:** **_allows_ new fields** to be added\n",
    "- Useful when schema changes in production/new fields added to nested data\n",
    "- **_`Cannot use`_** evolution to **_remove fields_**\n",
    "- All previous records will show newly added field as **_`Null`_**\n",
    "  - For previously written records, the underlying file isnâ€™t modified.\n",
    "  - The additional field is simply defined in the metadata and dynamically read as null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6213e4e3-41db-4dd2-a5fe-3cc4309ef330",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Alternative Quality Check Approaches\n",
    "- Add a **\"validation\" field** that captures any **validation errors** and a **null** value means validation passed.\n",
    "  - E.g. outlier or unexpected value is catured in validation field = passed for later reviewing\n",
    "- Quarantine data by filtering **non-compliant data** to alternate location\n",
    "  - E.g. Add quarantine table where instead dropping the record and omitting it from the final production table, we can pass it into separate table that can be processed later\n",
    "- Warn without failing by **_writing_ additional fields** with constraint check results to Delta tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27164d9a-b66b-4cf9-b97e-928d06333e4a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Pattern: Quarantine Invalid Records\n",
    "What if we want to save the records that violate data quality constraints for analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d944202-1b4b-445b-a2df-c457446f288e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Method 1: Create Inverse Expectation Rules\n",
    "\n",
    "<img src=\"./images/02_Streamline_ETL_With_DLT/Method_1.png\" alt=\"Method_1\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "**Limitations:**\n",
    "- Processes the data twice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dbb81b1-5356-40a9-a4b9-2a01f7eedf75",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Method 2: Add a validation status column and use for partitioning\n",
    "\n",
    "<img src=\"./images/02_Streamline_ETL_With_DLT/Method_2.png\" alt=\"Method_2\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "**Limitations:**\n",
    "- **_Doesn't use_ expectations**; **data quality metrics** are **_`not available`_** in the event logs or the pipelines UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73d0b046-c59d-40b0-8ea1-128ed45eacb9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Pattern: Verify Data with Row Comparison\n",
    "Validate **row counts** across tables to verify that data was processed successfully without dropping rows.\n",
    "**Solution:**\n",
    "- Add an **additional table** to your pipeline that defines an expectation to perform the comparison.\n",
    "- The results of this expectation appear in the **event log** and the **Delta Live Tables UI**.\n",
    "\n",
    "<img src=\"./images/02_Streamline_ETL_With_DLT/Pattern_Verify_Data_With_Row_Comparison.png\" alt=\"Pattern_Verify_Data_With_Row_Comparison\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f96b694d-e4ec-407f-919a-af967abf5d1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Pattern: Define Tables for Adv. Validation\n",
    "Perform advanced data validation with DLT expectations\n",
    "- Complex data quality checks examples\n",
    "  - A derived table contains **`all records`** from the source table\n",
    "  - Guaranteeing the **equality** of a **`numeric column`** across tables\n",
    "- Solution:\n",
    "  - Define DLT using **aggregate** and **join queries** and use the results of those queries as part of your expectation checking.\n",
    "\n",
    "<img src=\"./images/02_Streamline_ETL_With_DLT/Pattern_Define_Tables_For_Adv_Validation.png\" alt=\"Pattern_Define_Tables_For_Adv_Validation\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1136e775-e2e9-4355-858e-42c683f38cc1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b4ed17a-886d-4e1f-b0b5-d01e331fe6c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Dimensional Modeling\n",
    "**1. Fact Tables vs. Dimension Tables:**\n",
    "- **Fact Tables:** Often contain a **granular record** of activities\n",
    "- **Dimension Tables:** Often contain data may be **_updated_** or **_modified_** over time.\n",
    "\n",
    "<img src=\"./images/02_Streamline_ETL_With_DLT/Combined_Table.png\" alt=\"Combined_Table\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "**2. Modeling Guidelines:**\n",
    "- **_Denormalize_** dimension and fact tables\n",
    "- Use **conformed dimensions**\n",
    "- Use **slowly changing dimensions** as necessary\n",
    "\n",
    "**3. Fact Tables as Incremental Data:**\n",
    "- Often is a **time series**\n",
    "- No intermediate aggregations\n",
    "- No overwrite/update/delete operations\n",
    "- Often **`append-only`** operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd90796d-614e-4851-9d61-a17ece1bc37e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Slowly Changing Dimensions (SCD)\n",
    "\n",
    "**Type 0:**\n",
    "- **`No changes`** allowed\n",
    "- Tables are either **static** or **append** only\n",
    "- E.g. static lookup tables, append-only fact tables\n",
    "\n",
    "**Type 1:**\n",
    "- **`Overwrite`** but **`no history`** is maintained\n",
    "- May contain recording of when record was entered, but **`not` previous values**\n",
    "- E.g. valid customer mailing address\n",
    "\n",
    "**Type 2:**\n",
    "- Add a **`new row`**; mark old row as **obsolete**\n",
    "- **`Strong history`** is maintained\n",
    "- E.g. tracking product price changes over time\n",
    "\n",
    "<img src=\"./images/02_Streamline_ETL_With_DLT/SCD_123_Example.png\" alt=\"SCD_123_Example\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85ac9094-1aae-480c-b161-0fc3e69687c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## SCD Type 2 with DLT\n",
    "**Keep a record** of how values changed over time\n",
    "```\n",
    "APPLY CHANGES INTO LIVE.cities\n",
    "FROM STREAM(LIVE.city_updates)\n",
    "KEYS (id)\n",
    "SEQUENCE BY ts\n",
    "STORED AS SCD TYPE 2\n",
    "```\n",
    "- `__starts_at` and `__ends_at` will take on the type of the `SEQUENCE BY` field `ts`.\n",
    "\n",
    "<img src=\"./images/02_Streamline_ETL_With_DLT/SCD2_With_DLT.png\" alt=\"SCD2_With_DLT\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d6c6f58-1bf6-4330-9494-7439d7754824",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "APPLY CHANGES INTO LIVE.cities\n",
    "FROM STREAM(LIVE.city_updates)\n",
    "KEYS (id)\n",
    "SEQUENCE BY ts\n",
    "STORED AS SCD TYPE 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b521d49d-895c-4694-a39a-ed86b5c225ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Applying SCD Principles to Facts\n",
    "- Fact table usually **`append-only`** (Type 0)\n",
    "- Can leverage event and processing times for append-only history\n",
    "\n",
    "<img src=\"./images/02_Streamline_ETL_With_DLT/SCD_Principles_To_Facts.png\" alt=\"SCD_Principles_To_Facts\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4043740a-4dcc-47f7-9360-030fc3ca6353",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Streaming Joins and Statefulness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0134b2ef-5148-450b-a2c9-e78811ad4f17",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## The Components of a Stateful Stream\n",
    "\n",
    "<img src=\"./images/02_Streamline_ETL_With_DLT/Components_Stateful_Stream.png\" alt=\"Components_Stateful_Stream\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e6fdea2-dacf-43f1-9fd3-3fd04c1025c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Statefulness vs. Query Progress\n",
    "- Some operations are specifically **`stateful`** in that the results of processing **earlier records** from the stream **_affect_** the processing of **later records**.\n",
    "  - Examples include deduplication, aggregation, and stream-stream joins\n",
    "- Other transformations just need to **_store incremental_** query progress and are **`not stateful`**.\n",
    "  - Examples include simple transformations and stream-static joins\n",
    "- **Progress** and **state** are **_stored_ in `checkpoints`** and managed by the driver during query processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2df5496f-e3e9-4021-8a24-d26738d8809a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Stream-Static Joins\n",
    "Using **Dimension Tables** in Incremental Updates\n",
    "- Delta Lake enables **dynamic stream-static joins**\n",
    "- Each **`micro-batch`** captures the **most recent state** of the Delta table that is the static side of the join\n",
    "  - This does **`not occur`** if the static side of the join is another format such as **Parquet**\n",
    "- Allows modification of dimension while maintaining downstream\n",
    "composability\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> Because Delta Lake does not enforce foreign key constraints, it is possible that joined data will go unmatched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ec6458c-7396-44af-8c1e-85051a3e18b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Streaming Queries are Not Stateful\n",
    "**1. Each input row is processed only once**\n",
    "- **A change** to a **`streaming live`** table's definition **_does not recompute_** results by default:\n",
    "\n",
    "<img src=\"./images/02_Streamline_ETL_With_DLT/Not_Stateful_1.png\" alt=\"Not_Stateful_1\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "**2. Enrich data by joining with an up-to date-snapshot stored in Delta**\n",
    "- **A change** to **`joined table snapshot`** **_does not recompute_** results by default:\n",
    "\n",
    "<img src=\"./images/02_Streamline_ETL_With_DLT/Not_Stateful_2.png\" alt=\"Not_Stateful_2\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "671e8983-3311-48a7-89b8-451de31074c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Clear State in DLT\n",
    "Perform **`backfills`** after critical changes using **full refresh**\n",
    "- **Full-refresh** clears the tableâ€™s data and the queries state, **_reprocessing_ `all the data`**.\n",
    "\n",
    "<img src=\"./images/02_Streamline_ETL_With_DLT/Clear_State_DLT.png\" alt=\"Clear_State_DLT\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c37f3ad2-dcab-403b-a528-6fcf8e9d5da8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Stream-Static Join & Merge\n",
    "Join driven by streaming data\n",
    "- Join triggers shuffle\n",
    "- Join itself is stateless\n",
    "- Control state information with predicate\n",
    "- Goal is to broadcast static table to streaming data\n",
    "- Broadcasting puts all data on each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97ef794f-a0c4-4e4e-885f-0028b092142f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Main input stream\n",
    "salesSDF = (\n",
    " spark\n",
    " .readStream\n",
    " .format(\"delta\")\n",
    " .table(\"sales\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d59fcf8-4cf2-4076-b24e-2025c4f73c38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Join item category lookup\n",
    "itemSalesSDF = (\n",
    " salesSDF\n",
    " .join(\n",
    " spark.table(\"items\")\n",
    " .filter(\"category='Food'\"), #Predicate\n",
    " on=[\"item_id\"]\n",
    " )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Streamline_ETL_With_DLT",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
