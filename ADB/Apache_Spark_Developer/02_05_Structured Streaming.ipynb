{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f733e8f4-f978-420b-a665-05b8b7ca209c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 5. Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87204f55-f50c-4948-b8b3-e65956396f58",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Streaming Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd9f3da8-0584-4cad-8e4e-36f96b1772c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Sources:\n",
    "- Kafka, Files, Event Hubs, Kinesis\n",
    "- DataFrame\n",
    "  - ```\n",
    "    df = (spark.readStream\n",
    "              .option(\"maxFilesPerTrigger\", 1)\n",
    "              .format(\"delta\")\n",
    "              .load(DA.paths.events)\n",
    "            )\n",
    "    df.isStreaming\n",
    "    ```\n",
    "- SQL Views & Tables\n",
    "  - ```\n",
    "    df.createOrReplaceTempView(\"v_event\")\n",
    "    spark.readStream.format(\"delta\").table(\"v_event\")\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2685f6a4-995b-452d-bc7d-a118f6c2f7fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Sinks\n",
    "- **Where** to write data: Kafka, Files, Event Hubs/EventGrid, Foreach(Batch) for custom logic to store data\n",
    "- **What** data to write (Output Modes)\n",
    "  - APPEND: **add new** records only\n",
    "  - UPDATE **update changed** records in place\n",
    "    - Only rows updated since last trigger written\n",
    "    - Different from **Complete** mode since **Update** mode outputs only changed rows since last trigger\n",
    "    - If query does not contain aggregations, **Update** same as **Append** mode\n",
    "  - COMPLETE: **rewrite** full output\n",
    "  - Example:\n",
    "  \n",
    "    ![Output Modes](./images/Output_Modes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c3ce670-4fd3-427b-b16a-941a88d338ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Trigger Types\n",
    "- Default: Process each micro-batch as soon as previous one has been processed (or 500ms). *No coding required for this*. **Not recommended**\n",
    "- Fixed interval: Micro-batch processing kicked off at the **user-specified interval** \n",
    "  - `.trigger(processingTime=\"1 second\")` = every 1 second, bring new data\n",
    "- One-time: Process **all** available data as **a single micro-batch** and then automatically stop the query \n",
    "  - `.trigger(once=True)` for manually trigger\n",
    "- AvailableNow: Like Trigger One, available data processed before query stops, but in **multiple batches** instead of one \n",
    "  - `.trigger(availableNow=True)` **--> Recommended**\n",
    "- ContinuousProcessing: Long-running tasks that **continuously read, process and write** data as soon events are available \n",
    "  - `.trigger(continuous=\"1 second\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50803f52-1c04-470c-90db-8dbbfbb566a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### End-to-end fault tolerance\n",
    "Guaranteed in Structure Streaming by:\n",
    "1. Checkpointing: Directed Acyclic Graph (DAG) of all DStream transformation stored in reliable storage (along with optional State)\n",
    "    - `dbultils.fs.ls(checkpoitPath)`\n",
    "2. Write-ahead logs: To commit offsets\n",
    "    - Before it reads data into RAM, it writes data into disk system. Then it commits offsets. Hence, it knows where it left off in case there are interuption, then when the stream turn on, it switch back to where is left.\n",
    "3. Idempoten sinks: Writes given row only once, even if sent multiple\n",
    "4. Replayable data sources: Join allowed to poll data again\n",
    "\n",
    "![Checkpointing](./images/Checkpointing.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f19dad27-fb38-4a95-823d-7da97ac05b00",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Example: Complete Stream Query\n",
    "![Complete Streaming Query](./images/Complete_Streaming_Query.png)\n",
    "\n",
    "**Step 1:** Read Stream (lazy)\n",
    "  - `.option(\"maxFilesPerTrigger\", 1)` - how much data to read, in this case 1 file at a time\n",
    "    - It helps not overrun resource\n",
    "    - We can also use `.option(\"maxBytesPerTrigger\", 1000)`\n",
    "  - All of them will be held in RAM\n",
    "\n",
    "**Step 2:** Transformation\n",
    "\n",
    "**Step 3:** Write Stream (lazy)\n",
    "  - `emailTrafficDF.writeStream` write the DF in step 2 to disk\n",
    "  - `querryName(\"email_traffic)` the name of this query, It is good to provide name for query.\n",
    "  - `option(\"checkpointLocation\", checkpointPath)` it is **mandatory**\n",
    "  - `start(outputPath)` write into directory. `start` is **Action**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e2a3f3b-92d4-4676-b367-88a9db298a8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Streaming Best Practices\n",
    "1. Select trigger interval over nothing to unintended cost\n",
    "2. Use ADLS Gen2 > Blob storage for Azure\n",
    "3. Name the Streaming job so it's easily identifiable\n",
    "4. Don't run multitple Stream on the same Driver. Multiplexing on same cluster is generally not recommended\n",
    "5. Alter `maxFilesPerTrigger` or `maxBytesPerTrigger` to achieve Partition sized around **128MB - 200MB** (for best latency and throughput)\n",
    "6. Can convert `SortMergeJoin` to `BroadcastHashJoin`. May need to increase auto-broadcast hash join threshold to larger size\n",
    "7. If have Shuffle, consider setting Shuffle Partition number manually (since AQE is disabled in Streaming) to match number of Cores or 2x number of Cores\n",
    "8. Turn off Stats collection on initial Stream to decrease latency\n",
    "9. If possible, Auto-Optimize initial Stream to coalesce tiny files\n",
    "10. Use compute-optimized workers and RocksDB state store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e21ac4fa-99cc-4b08-961d-7fd0ef3a3f14",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Streaming Aggregates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1436632c-a7fa-46bd-8d51-501407c22b7a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Time-Based Windows\n",
    "- Tumbling Windows: any given event gets aggregated into only **one** window group. **No overlap**\n",
    "  - e.g. 1:00-1:05 am, 1:05-1:10am, 1:10-1:15am\n",
    "  - ```\n",
    "    .groupBy(window(\"eventtime\", \"5 minutes\"))\n",
    "    .count()\n",
    "    ```\n",
    "- Sliding Windows: any given event gets aggregated into **multiple** window groups. **Overlap**\n",
    "  - e.g. 1:00-1:10 am, 1:05-1:15am, 1:10-1:20am\n",
    "  - ```\n",
    "    .groupBy(window(\"eventtime\", \"10 minutes\", \"5 minutes\"))\n",
    "    .count()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5036be90-1f65-4525-87fb-bbcaa9943520",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Tumbling Windows\n",
    "**Example:**\n",
    "![Tumbling Window Example](./images/Tumbling_Window_Example.png)\n",
    "- For each device and every 1 hour, aggregate data (count)\n",
    "- The window() function returns **`window column`** which has **struct** datatype\n",
    "  - 2 subfields: `start` and `end`\n",
    "  - Parse them to get 3NF, using `windown.start` and `window.end`\n",
    "\n",
    "    ![Parse Start End Window](./images/Parse_Start_End_Window.png)\n",
    "- 200 tasks for Stage 13: because **`groupBy`** and **`count`**. Hence, it\n",
    "  - does **Wide Transformation** and the default shuffle partition is 200\n",
    "  - Then we can manually set partition to 4 or 8 (= number of Cores or double)\n",
    "\n",
    "    ![Manually Set Shuffle Partition To 4](./images/Manually_Set_Shuffle_Partition_To_4.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e56a43d5-40bf-414e-b4ce-b91b523106a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Sliding Windows\n",
    "**Example:**\n",
    "![Sliding Window Example](./images/Sliding_Window_Example.png)\n",
    "\n",
    "- Every 5 mins interval, go 10 mins and aggreate data (count)\n",
    "- Event 12:00-12:05: 2 *receipt* times \n",
    "  - 12:02: 1 cat, 1 dog\n",
    "  - 12:03: 2 dogs\n",
    "  - **Window 12:00-12:10:** 1 cat, 3 dogs\n",
    "- Event 12:05-12:10: 1 *receipt* time \n",
    "  - 12:07: 1 owl 1 cat\n",
    "  - **Window 12:00-12:10:** 2 cats, 3 dogs, 1 owl. **It was updated.**\n",
    "  - **Window 12:05-12:15:** 1 cat, 1 owl\n",
    "- Event 12:10-12:15: 2 *receipt* times\n",
    "  - 12:11: 1 dog\n",
    "  - 12:13: 1 owl\n",
    "  - **Window 12:00-12:10:** 2 cats, 3 dogs, 1 owl\n",
    "  - **Window 12:05-12:15:** 1 cat, 1 dog, 2 owl. **It was updated.**\n",
    "  - **Window 12:10-12:20:** 1 cat, 1 owl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05e608e3-0320-48c7-883f-6e0c987ecfae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Watermarks\n",
    "- Handle late data and limit how long to remember old data\n",
    "\n",
    "**Example:**\n",
    "\n",
    "![Watermark Handle Late Data](./images/Watermark_Handle_Late_Data.png)\n",
    "- Get back to the above example\n",
    "- Event 12:00-12:05: 2 *receipt* times \n",
    "  - 12:02: 1 cat, 1 dog\n",
    "  - 12:03: 2 dogs\n",
    "  - No **`12:04: 1 dog`** event arrived\n",
    "  - **Window 12:00-12:10:** 1 cat, 3 dogs\n",
    "- Event 12:05-12:10: 1 *receipt* time \n",
    "  - 12:07: 1 owl 1 cat\n",
    "  - **Window 12:00-12:10:** 2 cats, 3 dogs, 1 owl. **It was updated.**\n",
    "  - **Window 12:05-12:15:** 1 cat, 1 owl\n",
    "- Event 12:10-12:15: 2 *receipt* times\n",
    "  - **12:04: 1 dog**\n",
    "  - 12:13: 1 owl\n",
    "  - **Window 12:00-12:10:** 2 cats, **4** dogs, 1 owl. **It was updated, get late data.**\n",
    "  - **Window 12:05-12:15:** 1 cat, 2 owls\n",
    "  - **Window 12:10-12:20:** 1 owl\n",
    "\n",
    "**Example Script:** keep it for 2 hours ```.withWathermark(\"column\", \"2 hours\")```\n",
    "\n",
    "![Watermarking Script](./images/Watermarking_Script.png)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_05_Structured Streaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
