{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f733e8f4-f978-420b-a665-05b8b7ca209c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 5. Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87204f55-f50c-4948-b8b3-e65956396f58",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Streaming Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd9f3da8-0584-4cad-8e4e-36f96b1772c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Sources:\n",
    "- Kafka, Files, Event Hubs, Kinesis\n",
    "- DataFrame\n",
    "  - ```\n",
    "    df = (spark.readStream\n",
    "              .option(\"maxFilesPerTrigger\", 1)\n",
    "              .format(\"delta\")\n",
    "              .load(DA.paths.events)\n",
    "            )\n",
    "    df.isStreaming\n",
    "    ```\n",
    "- SQL Views & Tables\n",
    "  - ```\n",
    "    df.createOrReplaceTempView(\"v_event\")\n",
    "    spark.readStream.format(\"delta\").table(\"v_event\")\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2685f6a4-995b-452d-bc7d-a118f6c2f7fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Sinks\n",
    "- **Where** to write data: Kafka, Files, Event Hubs/EventGrid, Foreach(Batch) for custom logic to store data\n",
    "- **What** data to write (Output Modes)\n",
    "  - APPEND: **add new** records only\n",
    "  - UPDATE **update changed** records in place\n",
    "    - Only rows updated since last trigger written\n",
    "    - Different from **Complete** mode since **Update** mode outputs only changed rows since last trigger\n",
    "    - If query does not contain aggregations, **Update** same as **Append** mode\n",
    "  - COMPLETE: **rewrite** full output\n",
    "  - Example:\n",
    "  \n",
    "    ![Output Modes](./images/Output_Modes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c3ce670-4fd3-427b-b16a-941a88d338ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Trigger Types\n",
    "- Default: Process each micro-batch as soon as previous one has been processed (or 500ms). *No coding required for this*. **Not recommended**\n",
    "- Fixed interval: Micro-batch processing kicked off at the **user-specified interval** \n",
    "  - `.trigger(processingTime=\"1 second\")` = every 1 second, bring new data\n",
    "- One-time: Process **all** available data as **a single micro-batch** and then automatically stop the query \n",
    "  - `.trigger(once=True)` for manually trigger\n",
    "- AvailableNow: Like Trigger One, available data processed before query stops, but in **multiple batches** instead of one \n",
    "  - `.trigger(availableNow=True)` **--> Recommended**\n",
    "- ContinuousProcessing: Long-running tasks that **continuously read, process and write** data as soon events are available \n",
    "  - `.trigger(continuous=\"1 second\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50803f52-1c04-470c-90db-8dbbfbb566a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### End-to-end fault tolerance\n",
    "Guaranteed in Structure Streaming by:\n",
    "1. Checkpointing: Directed Acyclic Graph (DAG) of all DStream transformation stored in reliable storage (along with optional State)\n",
    "    - `dbultils.fs.ls(checkpoitPath)`\n",
    "2. Write-ahead logs: To commit offsets\n",
    "    - Before it reads data into RAM, it writes data into disk system. Then it commits offsets. Hence, it knows where it left off in case there are interuption, then when the stream turn on, it switch back to where is left.\n",
    "3. Idempoten sinks: Writes given row only once, even if sent multiple\n",
    "4. Replayable data sources: Join allowed to poll data again\n",
    "\n",
    "![Checkpointing](./images/Checkpointing.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f19dad27-fb38-4a95-823d-7da97ac05b00",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Example: Complete Stream Query\n",
    "![Complete Streaming Query](./images/Complete_Streaming_Query.png)\n",
    "\n",
    "**Step 1:** Read Stream (lazy)\n",
    "  - `.option(\"maxFilesPerTrigger\", 1)` - how much data to read, in this case 1 file at a time\n",
    "    - It helps not overrun resource\n",
    "    - We can also use `.option(\"maxBytesPerTrigger\", 1000)`\n",
    "  - All of them will be held in RAM\n",
    "\n",
    "**Step 2:** Transformation\n",
    "\n",
    "**Step 3:** Write Stream (lazy)\n",
    "  - `emailTrafficDF.writeStream` write the DF in step 2 to disk\n",
    "  - `querryName(\"email_traffic)` the name of this query, It is good to provide name for query.\n",
    "  - `option(\"checkpointLocation\", checkpointPath)` it is **mandatory**\n",
    "  - `start(outputPath)` write into directory. `start` is **Action**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e2a3f3b-92d4-4676-b367-88a9db298a8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Streaming Best Practices\n",
    "1. Select trigger interval over nothing to unintended cost\n",
    "2. Use ADLS Gen2 > Blob storage for Azure\n",
    "3. Name the Streaming job so it's easily identifiable\n",
    "4. Don't run multitple Stream on the same Driver. Multiplexing on same cluster is generally not recommended\n",
    "5. Alter `maxFilesPerTrigger` or `maxBytesPerTrigger` to achieve Partition sized around **128MB - 200MB** (for best latency and throughput)\n",
    "6. Can convert `SortMergeJoin` to `BroadcastHashJoin`. May need to increase auto-broadcast hash join threshold to larger size\n",
    "7. If have Shuffle, consider setting Shuffle Partition number manually (since AQE is disabled in Streaming) to match number of Cores or 2x number of Cores\n",
    "8. Turn off Stats collection on initial Stream to decrease latency\n",
    "9. If possible, Auto-Optimize initial Stream to coalesce tiny files\n",
    "10. Use compute-optimized workers and RocksDB state store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e21ac4fa-99cc-4b08-961d-7fd0ef3a3f14",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Streaming Aggregates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1436632c-a7fa-46bd-8d51-501407c22b7a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "abc"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_05_Structured Streaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
