{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93166f77-66d3-43fd-bcd4-dfb4cf4f1476",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 1. Databricks and Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74862d66-c1fc-4b37-96d6-79333e23130a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- **Row-Oriented data on disk (CSV):** data is stored in row wise basis (each record, ordered by column)\n",
    "- **Column-Oriented data on disk (Parquet, Delta, ORC):** data is stored in column wise basis (all data in column 1, all data in column 2... ordered by row)\n",
    "  - When we want to query column 1, 2, 3 then it only fetch those 3 column, not all like Row-Orientied\n",
    "  - It keeps statistics, metadata, etc. has compression built-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed27b870-d18c-4601-aa11-2442229add9c",
     "showTitle": true,
     "title": "Parameterize via Widgets - Python"
    }
   },
   "outputs": [],
   "source": [
    "dbultils.widgets.text(\"Text\", \"Hello World!\")\n",
    "dbultils.widgets.dropdown(\"Dropdown\", \"1\", [str (x) for x in rang(1,10)])\n",
    "dbultils.widgets.combobox(\"Combobox\", \"A\", [\"A\", \"B\", \"C\"])\n",
    "dbultils.widgets.multiselect(\"Multiselect\", \"Yes\", [\"Yes\", \"No\", \"Maybe\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "838d37fc-60bb-45eb-bf30-0d2b20b4cd74",
     "showTitle": true,
     "title": "Parameterize via Widgets - SQL"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE WIDGET TEXT state DEFAULT \"CA\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acd2f3ce-9d78-4959-8691-b49908dc1c29",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2. Apache Spark Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3176d0a9-57b4-46fb-b3b3-afa7595524b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## SparkSQL\n",
    "- It is a **module** for Structured data processing with multiple interfaces.\n",
    "- It includes any object that has a Schema or Structure, including SQL tables, DataFrames API for Python, Scala, Java and R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22503fe6-da15-487e-9095-161597f74ffd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Transformations\n",
    "- DataFrame Transformations are **lazily** eveluated (Job won't start until having **Action**)\n",
    "  - Schema eagerly evaludated by Driver, but Job not spawned\n",
    "  - Benefit of \"Lazy Evaluation\": Spark can make Optimization decisions after it look at the DAG (Directed Acyclic Graph)\n",
    "- Actions: are methods that trigger\n",
    "  - Job is spawned\n",
    "  - Examples: df.count(), df.collect(), df.show(), display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd01cd26-c554-499c-8b97-f72109dcdd3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## DataFrameReader\n",
    "- Interface used to load a DataFrame from external storage\n",
    "  - ```spark.read.csv(\"/Filestore/tables/LifeExp_headers.csv\")```\n",
    "- Explicit vs Implicit vs Infer Schema\n",
    "  1. **Explicitly** define Schema _**without reading**_ data files\n",
    "      ```\n",
    "      DDL_schema = (\"coutry STRING, lifeexp DOUBLE, region STRING)\n",
    "      userDF = spark.read.option(\"header\", True).schema(DDL_schema).csv(\"/Filestore/tables/LifeExp_headers.csv\")\n",
    "      ```\n",
    "  2. **Implicitly** create default Column names and Data types _**without reading**_ data files\n",
    "      ```\n",
    "      df1 = spark.read.load(\"/Filestore/tables/LifeExp_headers.csv\", format = \"csv\", header = False)\n",
    "      display(df1)\n",
    "      ```\n",
    "  3. **Infer** column names and data types _**by reading**_ data files\n",
    "      ```\n",
    "      df2 = spark.read.load(\"/Filestore/tables/LifeExp_headers.csv\", format = \"csv\", header = True, inferSchema = True)\n",
    "      display(df2)\n",
    "      ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd0d6f4d-534d-44bf-9b66-fcf710ea585e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## DataFrameWriter\n",
    "- Write DataFrame to external storage\n",
    "    ```\n",
    "    df.write\n",
    "      .format(\"delta)\n",
    "      .mode(\"append\")\n",
    "      .save(outPath)\n",
    "    ```\n",
    "- Write as SQL table\n",
    "    ```\n",
    "    df.write\n",
    "      .mode(\"overwrite\")\n",
    "      .saveAsTable(\"evants_p\")\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d66dc21-50d8-4104-8637-71ef96d7bb96",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Query Execution\n",
    "We can express the same query using any interface. The Spark SQL engine generates the same query plan used to optimize and execute on our Spark cluster.\n",
    "\n",
    "![query execution engine](https://files.training.databricks.com/images/aspwd/spark_sql_query_execution_engine.png)\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> Resilient Distributed Datasets (RDDs) are the low-level representation of datasets processed by a Spark cluster. In early versions of Spark, you had to write <a href=\"https://spark.apache.org/docs/latest/rdd-programming-guide.html\" target=\"_blank\">code manipulating RDDs directly</a>. In modern versions of Spark you should instead use the higher-level DataFrame APIs, which Spark automatically compiles into low-level RDD operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec7423b9-8466-4323-aa81-f9cb53946ea6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 4. Spark Architect and Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9fa17c3-5ca7-49a0-bc62-3587b0f9ff9b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Clusters\n",
    "- Example with a Cluster has 1 Driver, 6 Workers. Each Worker has 1 Executor and 2 Cores.\n",
    "  - Driver: brain of cluster which allocate tasks and data to worker nodes\n",
    "  - Worker: receives tasks and data, performs and return result to Deiver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db942025-317a-4a19-b58e-b907bbf5258c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76936d44-88c2-4930-b799-b595ddd78a78",
     "showTitle": true,
     "title": "Narrow vs Wide Transformation"
    }
   },
   "source": [
    "### Narrow vs Wide Transformation\n",
    "- Narrow Transformation: 1-1 Partition\n",
    "  - select, filter, cast, union\n",
    "  - Start with 1 memory partition, do transformations and data stay within the **same** memory partition\n",
    "- Wide Transformation: Causes Shuffle/Exchange\n",
    "  - distinct, groupBy, sort, join\n",
    "  - Redistribute data and then create **new** memory partitions\n",
    "  - Redistibuting or re-partitioning data so the data is grouped differently across partitions\n",
    "    - Based on data size we may need to decrease/increase the number of Shuffle partitions via ```spark.sql.shuffle.patitions```\n",
    "\n",
    "![Narrow vs Wide Transformation](./images/Narrow_And_Wide_Transformation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc3f5e56-7b86-469f-b02a-8bbc3bc4011a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Process of Narrow Transformation\n",
    "Example: Filter color != \"brown\". We have 12-16 memory partitions of data\n",
    "- Step 1: Driver puts data files into 12-16 equal sized parition\n",
    "- Step 2: Driver allocates 12 partitions to 12 Cores, each Core gets 1 partition --> Opps we still have 4 partitions left\n",
    "- Step 3: 4 Cores finish early and return result to Driver. The other Cores are still processing\n",
    "- Step 4: Driver allocates 4 partitions for another iteration to the 4 Cores. The other Cores finish\n",
    "- Step 5: 4 Cores finish the 2nd iteration and return resul to Driver\n",
    "- Step 6: Driver collects the result and delivers to the client\n",
    "\n",
    "![Process of Narrow Transformation](./images/Narrow_Transformation_Example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4461b189-b10d-496d-95fb-32924e6999d3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Process of Wide Transformation\n",
    "Example: Count total rows for each color. We have **19.5 MiB** data size, with 6 initial partitions\n",
    "- Stage 1: Local cCount\n",
    "  - Step 1: Driver allocates 6 partitions to 6 Cores, each Core gets 1 partition\n",
    "  - Step 2: 6 Cores finish early and **write** the result into Disk in dictionary key:value, so the file is only **568B**. For example:\n",
    "    - Core 2: Red:3, Blue:5, Yellow:7\n",
    "    - Core 3: Red:4, Blue:4, Yellow:8\n",
    "    - ...\n",
    "    - Core 12: Red:7, Blue:5, Yellow:5\n",
    "- Stage 2: Global Count\n",
    "  - Step 1: Driver allocates Core 7 to read the counts and do the \"Global Count\", then send the result\n",
    "  - Step 2: Core 7 **read** and sum the Local Counts and return the result to Driver\n",
    "  - Step 3: Driver collects the result and delivers to the client\n",
    "\n",
    "![Process of Wide Transformation](./images/Wide_Transformation_Example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "081c36e9-2d22-4e1f-b94e-3a84a299969c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Performance and Query Optimization\n",
    "There are 5 plans:\n",
    "\n",
    "**Input: Query, no matter programming language -->**\n",
    "1. Unresolved Logical Plan\n",
    "    - Drive reviews and confirms the **```schema correct?```**\n",
    "\n",
    "2. Analyzed Logical Plan\n",
    "    - Drive looks at **[Metadata Catalog] for ANALYSIS** and **```add data types```** to the columns\n",
    "\n",
    "3. Optimized Logical Plan\n",
    "    - Driver looks at **[Catalyst Catalog] for LOGICAL OPTIMIZATION** and check to apply number of rules-based (Filter, Join, etc.) to determine whether to **```move Filter before Join?```**\n",
    "\n",
    "4. Physical Plan\n",
    "    - Driver comes up with several ways (plans) to address the query **for PHYSICAL PLANNING** to determine **```which Join strategy to use, Data Skipping, Predicate Pushdown?```**\n",
    "\n",
    "5. Selected Physical Plan\n",
    "    - Driver puts ways in **[Cost Model] for WHOLE-STAGE CODE GENERATION** to see which way (plan) needs lowest cost = the best\n",
    "    - The plan would be **```Java bytecode and sent to Executors```**\n",
    "\n",
    "**--> Output: RDD, no matter dataframe, sql table or view**\n",
    "\n",
    "![Query Optimization](./images/Query_Optimization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad1edcfc-b585-45fb-b8b0-73ce313bc55d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Explain\n",
    "![Explain](./images/Explain.png)\n",
    "\n",
    "For each step, read from bottom to top\n",
    "\n",
    "**Step 1: Parsed Logical Plan**\n",
    "- It is shown as the script is wrote: Inner Join then Filter\n",
    "- Check the schema, e.g. lastname, firstname. dept... (no data types)\n",
    "\n",
    "**Step 2: Analyzed Logical Plan**\n",
    "- Add data type to each column of schema in step 1\n",
    "- Add CAST to ```dept``` to double as it is string\n",
    "\n",
    "It does not display all possible plans, but a optimized plan\n",
    "\n",
    "**Step 3: Optimized Logical Plan**\n",
    "- Move the Filter before Join, to get less rows to join\n",
    "\n",
    "**Step 4: Physical PLan**\n",
    "- Pick the best plan: Filter before Join and choose \"Sort Merge Join\" instead of \"Inner Join\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b853a12-6f04-44cc-930e-d68cb6ef033c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Cost Based Optimization (CBO): Smaller Tables Join\n",
    "![Cost Based Optimization (CBO)](./images/CBO.png)\n",
    "\n",
    "- Enable configurations: CBO, join reorder\n",
    "- In script, (2) we join **```large```** table to **```medium```** table, then (1) we join **```small```** table\n",
    "- In CBO step, it chooses to:\n",
    "  - (1) get **```small```** table join with **```medium```** table then join with **```large```** table last\n",
    "  - Afterward, (2) it does shuffle/exchange\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fda882c7-3d65-49bd-bc8b-9d57ce32aee2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Adaptive Query Execution (AQE)\n",
    "![Adaptive Query Execution (AQE)](./images/AQE.png)\n",
    "- After RDD created, Spark will look at Statistics and shuffle count to see how big they are. Then, it will turn back to Analyzed Logical Plan to see whether it can fine-tune the number of shuffle.\n",
    "\n",
    "\n",
    "![With And Without AQE](./images/With_Without_AQE.png)\n",
    "- **Without AQE:** we (1) start with 4 memory partitions and (2) end up with 200 memory partitions. **_But why does it need to shuffle 200 partitions for only 568B?_**, which mean 2B foreach partition --> too small\n",
    "- **With AQE:** we (1) start with 4 memory partitions and (2) end up with 1 partition, and much faster, even without parallism\n",
    "\n",
    "\n",
    "![Number of Jobs of AQE](./images/AQE_Number_Of_Jobs.png)\n",
    "- Number of jobs **increase** from 1 to 3 (job #3, #4, #5)\n",
    "  - Job #3 filters at WHERE clause to reduce size.\n",
    "  - Job #4 with AQE, needs 1 Shuffle\n",
    "- Hence, it **has cost overhead** with it, but it has benefit in long run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52f1ed58-598e-45a8-8b7c-9ec00ca0dff1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Predicate Pushdown on RDDs\n",
    "- Predicate Pushdown is when the data source actively **limits the number** of rows returned to Spark reader vi SELECT/WHERE/FILTER\n",
    "- Predicate Pushdown filters the data in the database query,\n",
    "  - Reducing the number of entries retrieved from the source database and \n",
    "  - Improving query performance\n",
    "  - By default, the Spark Dataset API will automatically push down valid WHERE clauses to the database\n",
    "- **Cast** function cannot be Pushed down\n",
    "\n",
    "![Predicate Pushdown](./images/Predicate_Pushdown.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa928622-46c9-4552-9bf3-6ff9d087fa1f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Caching - Best Practices\n",
    "- Don't cach unless you're sure a DataFrame will be **used multiple times**\n",
    "  - e.g. EDA (Exploratory Data Analysis), ML traning dataset\n",
    "- Omit unneeded columns to reduce the storage footprint\n",
    "- After calling `.cache()` which is **lazy transformation**, ensure all partitions are accessed with **Action**\n",
    "  - e.g. `count()` - put RDD into Cache\n",
    "- Manually evict cache when not needed\n",
    "  - e.g. `unpersist()` - remove RDD from Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69bfea2f-b6d8-40da-83ad-9d4f4d5ae675",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Memory Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5afd407d-ae08-4e37-ab8a-5bcc46f91c46",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Guidelines\n",
    "- Error on the side of **too many small** than to few large Memory Partitions. If so large memory, then the core does not have enough memory, leading 2 possible consequences:\n",
    "  - Spill to disk, waiting for more RAM, then bring it back\n",
    "  - OOM: out of memory error\n",
    "- Sweet spot initial size: **128MB and 1GB**\n",
    "- Calculate the size of Shuffle partitions by dividing Shuffle stage input (4TB) by the target partition size (200MB).\n",
    "  - e.g. 4TB / 200MB = 20,000 Shuffle Partition count\n",
    "  - By default, it is 200 `spark.conf.get(\"spark.sql.shuffle.partitions\")`\n",
    "- Can manually set number of Shuffle Partitions on case-by-case basis\n",
    "  - `spark.conf.set(\"spark.sql.shuffle.partitions\", \"20000\")`\n",
    "  - This setting is Local for **1 session** only.\n",
    "\n",
    "![200_Default_Partitions](./images/200_Default_Partitions.png)\n",
    "- Example: \n",
    "  - It starts with 8 partitions, then spawns 200 shuffle partitions\n",
    "  - But there are only 42KB (~no thing) to write\n",
    "  - Even some of tasks which means memory partitions reside has 0 bytes (blank)\n",
    "  - It looks like AEQ turned off --> turn it on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48018e1d-0d5a-40fe-bf51-e6956ca027b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Cores in Cluster\n",
    "- Initially, the Driver determines the number of Memory Partitions and its size. It decides based on:\n",
    "  - Number of Cores in Cluster. \n",
    "  - More Cores, more Patitions.\n",
    "- Get to number of Cores by 2 ways:\n",
    "  - `sc.defaultParallelism` or `spark.sparkContext.defaultParallelism`\n",
    "  - Spark UI -> Cores\n",
    "  \n",
    "  ![Spark UI](./images/Spark_UI_Cores.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c484fd21-1e26-4b03-a7b3-45c326b2aedf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### No. of Memory Partition for DataFrame\n",
    "- If Memory Partitions are sized too large (> 1GB), we can manually change in No. of Partitions (to higher number) to get them into a more reasonable size rang (**128MB** to **1GB**)\n",
    "- AQE can resolve some Partitions issues\n",
    "  - e.g. for small dataset, AQE won't create default 200 Shuffle Partitions, but rather a far lower number\n",
    "- We need to **convert DataFrame into RDD** to get number of Partitions used for the DataFrame\n",
    "  - `df.rdd.getNumPartitions()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a475e43b-2dce-4310-987b-57880042fb56",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Re-Partition a DataFrame\n",
    "There are 2 ways:\n",
    "1. **`coalesce(int)`:**\n",
    "    - Returns new DF with exactly N partitions when N < current No. of Partitions\n",
    "    - **Narrow** transformation\n",
    "    - Pros:\n",
    "        - Retain sort order\n",
    "        - No shuffle\n",
    "    - Cons:\n",
    "        - Only decrease No. of Partitions\n",
    "        - Unevenly balanced partition sizes\n",
    "\n",
    "2. **`repartition(int, [col])`:**\n",
    "    - Return new DF with exactly N partitions\n",
    "    - **Wide** transformation\n",
    "    - Pros:\n",
    "        - Evenly balanced partition sizes\n",
    "        - Both increase/decrease No. of Partition\n",
    "    - Cons:\n",
    "        - Not retain sort order\n",
    "        - Require Shuffle\n",
    "\n",
    "**Notes:**\n",
    "  - More No. of Partition, less size\n",
    "  - Less No. of Paritions, more size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f733e8f4-f978-420b-a665-05b8b7ca209c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 5. Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87204f55-f50c-4948-b8b3-e65956396f58",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Streaming Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd9f3da8-0584-4cad-8e4e-36f96b1772c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Sources:\n",
    "- Kafka, Files, Event Hubs, Kinesis\n",
    "- DataFrame\n",
    "  - ```\n",
    "    df = (spark.readStream\n",
    "              .option(\"maxFilesPerTrigger\", 1)\n",
    "              .format(\"delta\")\n",
    "              .load(DA.paths.events)\n",
    "            )\n",
    "    df.isStreaming\n",
    "    ```\n",
    "- SQL Views & Tables\n",
    "  - ```\n",
    "    df.createOrReplaceTempView(\"v_event\")\n",
    "    spark.readStream.format(\"delta\").table(\"v_event\")\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2685f6a4-995b-452d-bc7d-a118f6c2f7fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Sinks\n",
    "- **Where** to write data: Kafka, Files, Event Hubs/EventGrid, Foreach(Batch) for custom logic to store data\n",
    "- **What** data to write (Output Modes)\n",
    "  - APPEND: **add new** records only\n",
    "  - UPDATE **update changed** records in place\n",
    "    - Only rows updated since last trigger written\n",
    "    - Different from **Complete** mode since **Update** mode outputs only changed rows since last trigger\n",
    "    - If query does not contain aggregations, **Update** same as **Append** mode\n",
    "  - COMPLETE: **rewrite** full output\n",
    "  - Example:\n",
    "  \n",
    "    ![Output Modes](./images/Output_Modes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c3ce670-4fd3-427b-b16a-941a88d338ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Trigger Types\n",
    "- Default: Process each micro-batch as soon as previous one has been processed (or 500ms). *No coding required for this*. **Not recommended**\n",
    "- Fixed interval: Micro-batch processing kicked off at the **user-specified interval** \n",
    "  - `.trigger(processingTime=\"1 second\")` = every 1 second, bring new data\n",
    "- One-time: Process **all** available data as **a single micro-batch** and then automatically stop the query \n",
    "  - `.trigger(once=True)` for manually trigger\n",
    "- AvailableNow: Like Trigger One, available data processed before query stops, but in **multiple batches** instead of one \n",
    "  - `.trigger(availableNow=True)` **--> Recommended**\n",
    "- ContinuousProcessing: Long-running tasks that **continuously read, process and write** data as soon events are available \n",
    "  - `.trigger(continuous=\"1 second\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50803f52-1c04-470c-90db-8dbbfbb566a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### End-to-end fault tolerance\n",
    "Guaranteed in Structure Streaming by:\n",
    "1. Checkpointing: Directed Acyclic Graph (DAG) of all DStream transformation stored in reliable storage (along with optional State)\n",
    "    - `dbultils.fs.ls(checkpoitPath)`\n",
    "2. Write-ahead logs: To commit offsets\n",
    "    - Before it reads data into RAM, it writes data into disk system. Then it commits offsets. Hence, it knows where it left off in case there are interuption, then when the stream turn on, it switch back to where is left.\n",
    "3. Idempoten sinks: Writes given row only once, even if sent multiple\n",
    "4. Replayable data sources: Join allowed to poll data again\n",
    "\n",
    "![Checkpointing](./images/Checkpointing.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f19dad27-fb38-4a95-823d-7da97ac05b00",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Example: Complete Stream Query\n",
    "![Complete Streaming Query](./images/Complete_Streaming_Query.png)\n",
    "\n",
    "**Step 1:** Read Stream (lazy)\n",
    "  - `.option(\"maxFilesPerTrigger\", 1)` - how much data to read, in this case 1 file at a time\n",
    "    - It helps not overrun resource\n",
    "    - We can also use `.option(\"maxBytesPerTrigger\", 1000)`\n",
    "  - All of them will be held in RAM\n",
    "\n",
    "**Step 2:** Transformation\n",
    "\n",
    "**Step 3:** Write Stream (lazy)\n",
    "  - `emailTrafficDF.writeStream` write the DF in step 2 to disk\n",
    "  - `querryName(\"email_traffic)` the name of this query, It is good to provide name for query.\n",
    "  - `option(\"checkpointLocation\", checkpointPath)` it is **mandatory**\n",
    "  - `start(outputPath)` write into directory. `start` is **Action**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e2a3f3b-92d4-4676-b367-88a9db298a8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Streaming Best Practices\n",
    "1. Select trigger interval over nothing to unintended cost\n",
    "2. Use ADLS Gen2 > Blob storage for Azure\n",
    "3. Name the Streaming job so it's easily identifiable\n",
    "4. Don't run multitple Stream on the same Driver. Multiplexing on same cluster is generally not recommended\n",
    "5. Alter `maxFilesPerTrigger` or `maxBytesPerTrigger` to achieve Partition sized around **128MB - 200MB** (for best latency and throughput)\n",
    "6. Can convert `SortMergeJoin` to `BroadcastHashJoin`. May need to increase auto-broadcast hash join threshold to larger size\n",
    "7. If have Shuffle, consider setting Shuffle Partition number manually (since AQE is disabled in Streaming) to match number of Cores or 2x number of Cores\n",
    "8. Turn off Stats collection on initial Stream to decrease latency\n",
    "9. If possible, Auto-Optimize initial Stream to coalesce tiny files\n",
    "10. Use compute-optimized workers and RocksDB state store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e21ac4fa-99cc-4b08-961d-7fd0ef3a3f14",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Streaming Aggregates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1436632c-a7fa-46bd-8d51-501407c22b7a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Apache_Spark_Programming_Clone",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
