{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93166f77-66d3-43fd-bcd4-dfb4cf4f1476",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 3. Functions in Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e53e771d-40f1-46c3-a994-718a05ac7125",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8143be98-9a17-46e7-9b3e-937a8aaaa740",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### \"summary\" and \"describe\"\n",
    "![Summary and Describe](./images/Summary_Describe.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d3b9aa3-2de1-4566-b0b9-274bf1b2fd92",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### format_number()\n",
    "![format_number](./images/format_number.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dc1a5d4-0951-4cfa-900f-3a71a8619ce7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### groupBy\n",
    "Use the DataFrame **`groupBy`** method to create a grouped data object. \n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/aspwd/aggregation_groupby.png\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe904b4c-ed64-4499-8841-43d5d4998e40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"event_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f4f5d7a-751a-4672-aab9-0f04ccf980f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"geo.state\", \"geo.city\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3808be2f-6405-4791-9c74-345a69c40880",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Grouped data methods\n",
    "Various aggregation methods are available on the <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/grouping.html\" target=\"_blank\">GroupedData</a> object.\n",
    "\n",
    "\n",
    "| Method | Description |\n",
    "| --- | --- |\n",
    "| agg | Compute aggregates by specifying a series of aggregate columns |\n",
    "| avg | Compute the mean value for each numeric columns for each group |\n",
    "| count | Count the number of rows for each group |\n",
    "| max | Compute the max value for each numeric columns for each group |\n",
    "| mean | Compute the average value for each numeric columns for each group |\n",
    "| min | Compute the min value for each numeric column for each group |\n",
    "| pivot | Pivots a column of the current DataFrame and performs the specified aggregation |\n",
    "| sum | Compute the sum for each numeric columns for each group |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c662727a-821d-4f87-9352-758b378a73d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here, we're getting the average purchase revenue for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "297ee493-ac05-49ae-9466-e3980810df02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "avg_state_purchases_df = df.groupBy(\"geo.state\").avg(\"ecommerce.purchase_revenue_in_usd\")\n",
    "display(avg_state_purchases_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5240d1a7-ff65-4fcb-8536-67ec1563a639",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "And here the total quantity and sum of the purchase revenue for each combination of state and city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0288bed2-8af4-4f75-b144-f17e7d828719",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "city_purchase_quantities_df = df.groupBy(\"geo.state\", \"geo.city\").sum(\"ecommerce.total_item_quantity\", \"ecommerce.purchase_revenue_in_usd\")\n",
    "display(city_purchase_quantities_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ec15a64-d996-4de3-ab6a-1bf2ac9f92b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Built-In Functions\n",
    "In addition to DataFrame and Column transformation methods, there are a ton of helpful functions in Spark's built-in <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-functions-builtin.html\" target=\"_blank\">SQL functions</a> module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "881c6831-70bf-43cb-9dd1-dd90cf9c1f3e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Aggregate Functions **`agg`**\n",
    "\n",
    "Here are some of the built-in functions available for aggregation.\n",
    "\n",
    "| Method | Description |\n",
    "| --- | --- |\n",
    "| approx_count_distinct | Returns the approximate number of distinct items in a group |\n",
    "| avg | Returns the average of the values in a group |\n",
    "| collect_list | Returns a list of objects with duplicates |\n",
    "| corr | Returns the Pearson Correlation Coefficient for two columns |\n",
    "| max | Compute the max value for each numeric columns for each group |\n",
    "| mean | Compute the average value for each numeric columns for each group |\n",
    "| stddev_samp | Returns the sample standard deviation of the expression in a group |\n",
    "| sumDistinct | Returns the sum of distinct values in the expression |\n",
    "| var_pop | Returns the population variance of the values in a group |\n",
    "\n",
    "Use the grouped data method <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.agg.html#pyspark.sql.GroupedData.agg\" target=\"_blank\">**`agg`**</a> to apply built-in aggregate functions\n",
    "\n",
    "This allows you to apply other transformations on the resulting columns, such as <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.alias.html\" target=\"_blank\">**`alias`**</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67215c87-6687-4ef8-b5d2-714ccc29c5d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "state_purchases_df = df.groupBy(\"geo.state\").agg(sum(\"ecommerce.total_item_quantity\").alias(\"total_purchases\"))\n",
    "display(state_purchases_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32ff77d4-dfc6-47cc-9268-52de763c2d56",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Apply multiple aggregate functions on grouped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad388d59-f601-4135-9c34-99d2767414b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, approx_count_distinct\n",
    "\n",
    "state_aggregates_df = (df\n",
    "                       .groupBy(\"geo.state\")\n",
    "                       .agg(avg(\"ecommerce.total_item_quantity\").alias(\"avg_quantity\"),\n",
    "                            approx_count_distinct(\"user_id\").alias(\"distinct_users\"))\n",
    "                      )\n",
    "\n",
    "display(state_aggregates_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6577c27-3261-41bc-9230-3af9145c05d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Math Functions\n",
    "Here are some of the built-in functions for math operations.\n",
    "\n",
    "| Method | Description |\n",
    "| --- | --- |\n",
    "| ceil | Computes the ceiling of the given column. |\n",
    "| cos | Computes the cosine of the given value. |\n",
    "| log | Computes the natural logarithm of the given value. |\n",
    "| round | Returns the value of the column e rounded to 0 decimal places with HALF_UP round mode. |\n",
    "| sqrt | Computes the square root of the specified float value. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4379443-a136-418f-b908-3abae9ffbbef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import cos, sqrt\n",
    "\n",
    "display(spark.range(10)  # Create a DataFrame with a single column called \"id\" with a range of integer values\n",
    "        .withColumn(\"sqrt\", sqrt(\"id\"))\n",
    "        .withColumn(\"cos\", cos(\"id\"))\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55235741-dc63-4240-8c0c-9a3086bc7a6a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## DateTimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f46b6d2-f342-4eeb-8362-5af43f8788ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Unix time\n",
    "- Unix time is a system for describing a point in time. It is the number of million seconds that have elapsed since Unix epoch. The Unix eposh is 00:00:00 UTC on 1 Jan 1970.\n",
    "  - ```\n",
    "    df2 = df1.withColumn(\"ts\", (col(\"unixtime\") / 1e6).cast(\"timestamp\"))\n",
    "    ```\n",
    "\n",
    "![Unix Timestamp](./images/Unix_Timestamp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "135b2274-b111-4d2b-b2a4-0e9dc0dff2e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "### Built-In Functions: Date Time Functions\n",
    "Here are a few built-in functions to manipulate dates and times in Spark.\n",
    "\n",
    "| Method | Description |\n",
    "| --- | --- |\n",
    "| **`add_months`** | Returns the date that is numMonths after startDate |\n",
    "| **`current_timestamp`** | Returns the current timestamp at the start of query evaluation as a timestamp column |\n",
    "| **`date_format`** | Converts a date/timestamp/string to a value of string in the format specified by the date format given by the second argument. |\n",
    "| **`dayofweek`** | Extracts the day of the month as an integer from a given date/timestamp/string |\n",
    "| **`from_unixtime`** | Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the yyyy-MM-dd HH:mm:ss format |\n",
    "| **`minute`** | Extracts the minutes as an integer from a given date/timestamp/string. |\n",
    "| **`unix_timestamp`** | Converts time string with given pattern to Unix timestamp (in seconds) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "117802dd-29ee-4654-a570-33019cb6ac7e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Datetime Patterns for Formatting and Parsing\n",
    "\n",
    "Spark uses <a href=\"https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\" target=\"_blank\">pattern letters for date and timestamp parsing and formatting</a>. A subset of these patterns are shown below.\n",
    "\n",
    "| Symbol | Meaning         | Presentation | Examples               |\n",
    "| ------ | --------------- | ------------ | ---------------------- |\n",
    "| G      | era             | text         | AD; Anno Domini        |\n",
    "| y      | year            | year         | 2020; 20               |\n",
    "| D      | day-of-year     | number(3)    | 189                    |\n",
    "| M/L    | month-of-year   | month        | 7; 07; Jul; July       |\n",
    "| d      | day-of-month    | number(3)    | 28                     |\n",
    "| Q/q    | quarter-of-year | number/text  | 3; 03; Q3; 3rd quarter |\n",
    "| E      | day-of-week     | text         | Tue; Tuesday           |\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_warn_32.png\" alt=\"Warning\"> Spark's handling of dates and timestamps changed in version 3.0, and the patterns used for parsing and formatting these values changed as well. For a discussion of these changes, please reference <a href=\"https://databricks.com/blog/2020/07/22/a-comprehensive-look-at-dates-and-timestamps-in-apache-spark-3-0.html\" target=\"_blank\">this Databricks blog post</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "756e0775-450a-405b-ba14-15d8677c1c7a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### **`date_format()`**\n",
    "Converts a date/timestamp/string to a string formatted with the given date time pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0623b9c-3a1a-423a-b9df-9b09ba6ff68e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "formatted_df = (timestamp_df\n",
    "                .withColumn(\"date string\", date_format(\"col_timestamp\", \"MMMM dd, yyyy\"))\n",
    "                .withColumn(\"time string\", date_format(\"col_timestamp\", \"HH:mm:ss.SSSSSS\"))\n",
    "               )\n",
    "display(formatted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74040f69-eb83-4f50-943c-d1514b3b842b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### **`cast()`**\n",
    "\n",
    "Casts column to a different data type, specified using string representation or DataType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8889586-08e6-423d-8b63-50d02a382640",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "timestamp_df = df.withColumn(\"col_timestamp\", (col(\"col_timestamp\") / 1e6).cast(\"timestamp\"))\n",
    "display(timestamp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7936c687-9e47-4521-82ef-ccef78073078",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### **`year`**\n",
    "Extracts the year as an integer from a given date/timestamp/string.\n",
    "\n",
    "##### Similar methods: **`month`**, **`dayofweek`**, **`minute`**, **`second`**, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2fad565-4b47-4fd7-8a26-fe3740823887",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofweek, minute, second\n",
    "\n",
    "datetime_df = (timestamp_df\n",
    "               .withColumn(\"year\", year(col(\"col_timestamp\")))\n",
    "               .withColumn(\"month\", month(col(\"col_timestamp\")))\n",
    "               .withColumn(\"dayofweek\", dayofweek(col(\"col_timestamp\")))\n",
    "               .withColumn(\"minute\", minute(col(\"col_timestamp\")))\n",
    "               .withColumn(\"second\", second(col(\"col_timestamp\")))\n",
    "              )\n",
    "display(datetime_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54842bd9-4f46-4f40-82a1-b09ecd4902a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### **`to_date`**\n",
    "Converts the column into DateType by casting rules to DateType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "265aea95-511b-4949-8d28-3829970a459a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "date_df = timestamp_df.withColumn(\"date\", to_date(col(\"col_timestamp\")))\n",
    "display(date_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14ffa2f5-fd66-49aa-b6bc-e5af625fbbd0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "#### **`date_add`**\n",
    "Returns the date that is the given number of days after start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39d14a54-4a9e-4954-8e0d-21b4cb0689be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add\n",
    "\n",
    "plus_2_df = timestamp_df.withColumn(\"plus_two_days\", date_add(col(\"timestamp\"), 2))\n",
    "display(plus_2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6bf8054-e946-4c06-b857-6b2cef3ceada",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Complex Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfa2bc2d-45c4-46da-87b5-e3c4bd650dcd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### String Functions\n",
    "Here are some of the built-in functions available for manipulating strings.\n",
    "\n",
    "| Method | Description |\n",
    "| --- | --- |\n",
    "| translate | Translate any character in the src by a character in replaceString |\n",
    "| regexp_replace | Replace all substrings of the specified string value that match regexp with rep |\n",
    "| regexp_extract | Extract a specific group matched by a Java regex, from the specified string column |\n",
    "| ltrim | Removes the leading space characters from the specified string column |\n",
    "| lower | Converts a string column to lowercase |\n",
    "| split | Splits str around matches of the given pattern |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "861f3708-c025-4968-8095-e44d01121868",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "For example: let's imagine that we need to parse our **`email`** column. We're going to use the **`split`** function  to split domain and handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4afb031-9448-4323-8701-a01c93258ebf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "display(df.select(split(df.email, '@', 0).alias('email_handle')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "976a7f3c-debd-448c-a39b-000fa405a447",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Collection Functions\n",
    "\n",
    "Here are some of the built-in functions available for working with arrays.\n",
    "\n",
    "| Method | Description |\n",
    "| --- | --- |\n",
    "| array_contains | Returns null if the array is null, true if the array contains value, and false otherwise. |\n",
    "| element_at | Returns element of array at given index. Array elements are numbered starting with **1**. |\n",
    "| explode | Creates a new row for each element in the given array or map column. |\n",
    "| collect_set | Returns a set of objects with duplicate elements eliminated. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5344204a-e3c6-4076-a8d7-6f22440497a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mattress_df = (details_df\n",
    "               .filter(array_contains(col(\"details\"), \"Mattress\"))\n",
    "               .withColumn(\"size\", element_at(col(\"details\"), 2)))\n",
    "display(mattress_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cd5c449-bd7d-42a0-9a02-583e55452baf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Aggregate Functions\n",
    "\n",
    "Here are some of the built-in aggregate functions available for creating arrays, typically from GroupedData.\n",
    "\n",
    "| Method | Description |\n",
    "| --- | --- |\n",
    "| collect_list | Returns an array consisting of all values within the group. |\n",
    "| collect_set | Returns an array consisting of all unique values within the group. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "759fe080-f522-4227-a2c5-1240504ea9b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Let's say that we wanted to see the sizes of mattresses ordered by each email address. For this, we can use the **`collect_set`** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4943625f-109d-4a17-95fe-2b638f6ea5ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "size_df = mattress_df.groupBy(\"email\").agg(collect_set(\"size\").alias(\"size options\"))\n",
    "\n",
    "display(size_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0550b74-b6ce-47fc-be05-a8d3a9ce2daa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Union and unionByName\n",
    "<img src=\"https://files.training.databricks.com/images/icon_warn_32.png\" alt=\"Warning\"> The DataFrame <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.union.html\" target=\"_blank\">**`union`**</a> method resolves columns by position, as in standard SQL. You should use it only if the two DataFrames have exactly the same schema, including the column order. In contrast, the DataFrame <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.unionByName.html\" target=\"_blank\">**`unionByName`**</a> method resolves columns by name.  This is equivalent to UNION ALL in SQL.  Neither one will remove duplicates.  \n",
    "\n",
    "Below is a check to see if the two dataframes have a matching schema where **`union`** would be appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cd5d8a0-c67d-4026-abe2-a9f0a46504d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mattress_df.schema==size_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc524b97-6140-405b-996c-1bbb63d23c95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "If we do get the two schemas to match with a simple select statement, then we can use a union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "207178e6-1584-47f2-96e1-ec41ec384505",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "union_count = mattress_df.select(\"email\").union(size_df.select(\"email\")).count()\n",
    "\n",
    "mattress_count = mattress_df.count()\n",
    "size_count = size_df.count()\n",
    "\n",
    "mattress_count + size_count == union_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7b7ee9e-1966-4799-8c4a-1d43b5c72803",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Additional Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f260973-b6ff-49f4-ad58-f6df307e5501",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Non-aggregate and Miscellaneous Functions\n",
    "Here are a few additional non-aggregate and miscellaneous built-in functions.\n",
    "\n",
    "| Method | Description |\n",
    "| --- | --- |\n",
    "| col / column | Returns a Column based on the given column name. |\n",
    "| lit | Creates a Column of literal value |\n",
    "| isnull | Return true iff the column is null |\n",
    "| rand | Generate a random column with independent and identically distributed (i.i.d.) samples uniformly distributed in [0.0, 1.0) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f0739d0-ea7d-491e-bfb4-029e9848dc12",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### **`col()`**\n",
    "\n",
    "We could select a particular column using the **`col`** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4524325d-740f-4f92-81b0-b08087ece8da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gmail_accounts = sales_df.filter(col(\"email\").endswith(\"gmail.com\"))\n",
    "\n",
    "display(gmail_accounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a900e6e4-a14f-4afb-bed8-10c89236b5bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### **`lit`** \n",
    "Used to create a column out of a value, which is useful for appending columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0b1f4ff-c277-4d98-b2a9-5620d0f1bc50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(gmail_accounts.select(\"email\", lit(True).alias(\"gmail user\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8eb679cc-c03e-4892-af53-e74354f0f280",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "### DataFrame Na Functions\n",
    "<a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameNaFunctions.html#pyspark.sql.DataFrameNaFunctions\" target=\"_blank\">DataFrameNaFunctions</a> is a DataFrame submodule with methods for handling null values. Obtain an instance of DataFrameNaFunctions by accessing the **`na`** attribute of a DataFrame.\n",
    "\n",
    "| Method | Description |\n",
    "| --- | --- |\n",
    "| drop | Returns a new DataFrame omitting rows with any, all, or a specified number of null values, considering an optional subset of columns |\n",
    "| fill | Replace null values with the specified value for an optional subset of columns |\n",
    "| replace | Returns a new DataFrame replacing a value with another value, considering an optional subset of columns |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c5a8574-e6f0-4dc4-b362-0adfc6bf6e0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(sales_df.count())\n",
    "print(sales_df.na.drop().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdbf58a2-de6f-4524-980a-cc47bab9119f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "We can fill in the missing coupon codes with **`na.fill`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48bc7208-3cb9-44e1-aa3d-af95b82f809c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(sales_exploded_df.select(\"items.coupon\").na.fill(\"NO COUPON\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6adfc55-1b0f-433b-99b4-433e694683f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "### Joining DataFrames\n",
    "The DataFrame <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.join.html?highlight=join#pyspark.sql.DataFrame.join\" target=\"_blank\">**`join`**</a> method joins two DataFrames based on a given join expression. \n",
    "\n",
    "Several different types of joins are supported:\n",
    "\n",
    "Inner join based on equal values of a shared column called \"name\" (i.e., an equi join)<br/>\n",
    "**`df1.join(df2, \"name\")`**\n",
    "\n",
    "Inner join based on equal values of the shared columns called \"name\" and \"age\"<br/>\n",
    "**`df1.join(df2, [\"name\", \"age\"])`**\n",
    "\n",
    "Full outer join based on equal values of a shared column called \"name\"<br/>\n",
    "**`df1.join(df2, \"name\", \"outer\")`**\n",
    "\n",
    "Left outer join based on an explicit column expression<br/>\n",
    "**`df1.join(df2, df1[\"customer_name\"] == df2[\"account_name\"], \"left_outer\")`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37b2a855-697d-495f-8238-60f8fbb025f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df = gmail_accounts.join(other=users_df, on='email', how = \"inner\")\n",
    "display(joined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a50a24c-2cae-4f2b-a397-420f09e468cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "## User-Defined Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "421bcf46-65af-4a28-9c5f-eafab5e26412",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "### User-Defined Function (UDF)\n",
    "A custom column transformation function\n",
    "\n",
    "- Can’t be optimized by Catalyst Optimizer\n",
    "- Function is serialized and sent to executors\n",
    "- Row data is deserialized from Spark's native binary format to pass to the UDF, and the results are serialized back into Spark's native format\n",
    "- For Python UDFs, additional interprocess communication overhead between the executor and a Python interpreter running on each worker node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d897518-5bc7-4251-bd65-d8d3cbca23f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Define a function\n",
    "\n",
    "Define a function (on the driver) to get the first letter of a string from the **`email`** field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dafb6058-e2cd-4dc7-8610-a977b186cd83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def first_letter_function(email):\n",
    "    return email[0]\n",
    "\n",
    "first_letter_function(\"annagray@kaufman.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f9fd71a-b0da-4d8d-805f-fa25dbd3a062",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Create and apply UDF\n",
    "Register the function as a UDF. This serializes the function and sends it to executors to be able to transform DataFrame records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96b5399c-129b-46a0-8353-27dd36b4e31c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "first_letter_udf = udf(first_letter_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1b1231a-d642-404f-b0d8-789475b1deea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Apply the UDF on the **`email`** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f1a6126-365a-4d78-ade8-1525d88a5baf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "display(sales_df.select(first_letter_udf(col(\"email\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cf615aa-cf50-4cf4-aae4-2ed98b4310f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Register UDF to use in SQL\n",
    "Register the UDF using **`spark.udf.register`** to also make it available for use in the SQL namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9af85d06-6ffa-4911-9b4c-be0d5e1d3272",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "first_letter_udf = spark.udf.register(\"sql_udf\", first_letter_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94b067d9-95b5-4a90-8610-ac8deed93c21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# You can still apply the UDF from Python\n",
    "display(sales_df.select(first_letter_udf(col(\"email\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37a30e30-b72d-43ab-906c-8b10d3beb72e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- You can now also apply the UDF from SQL\n",
    "SELECT sql_udf(email) AS first_letter FROM sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bafeaaff-788e-4378-b31a-f07ba64d86e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Use Decorator Syntax (Python Only)\n",
    "\n",
    "Alternatively, you can define and register a UDF using <a href=\"https://realpython.com/primer-on-python-decorators/\" target=\"_blank\">Python decorator syntax</a>. The **`@udf`** decorator parameter is the Column datatype the function returns.\n",
    "\n",
    "You will no longer be able to call the local Python function (i.e., **`first_letter_udf(\"annagray@kaufman.com\")`** will not work).\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> This example also uses <a href=\"https://docs.python.org/3/library/typing.html\" target=\"_blank\">Python type hints</a>, which were introduced in Python 3.5. Type hints are not required for this example, but instead serve as \"documentation\" to help developers use the function correctly. They are used in this example to emphasize that the UDF processes one record at a time, taking a single **`str`** argument and returning a **`str`** value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc124770-b603-4aca-8e0d-100568f29d8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Our input/output is a string\n",
    "@udf(\"string\")\n",
    "def first_letter_udf(email: str) -> str:\n",
    "    return email[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdafcd11-0e9f-4660-b65f-7329f91c9564",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "And let's use our decorator UDF here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "effec0c2-0a24-4061-85ba-4aae7e987865",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "sales_df = spark.read.format(\"delta\").load(DA.paths.sales)\n",
    "display(sales_df.select(first_letter_udf(col(\"email\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6918982e-5503-42d2-83c9-6d4f9aa0f154",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Pandas/Vectorized UDFs\n",
    "\n",
    "Pandas UDFs are available in Python to improve the efficiency of UDFs. Pandas UDFs utilize Apache Arrow to speed up computation.\n",
    "\n",
    "* <a href=\"https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html\" target=\"_blank\">Blog post</a>\n",
    "* <a href=\"https://spark.apache.org/docs/latest/api/python/user_guide/sql/arrow_pandas.html?highlight=arrow\" target=\"_blank\">Documentation</a>\n",
    "\n",
    "<img src=\"https://databricks.com/wp-content/uploads/2017/10/image1-4.png\" alt=\"Benchmark\" width =\"500\" height=\"1500\">\n",
    "\n",
    "The user-defined functions are executed using: \n",
    "* <a href=\"https://arrow.apache.org/\" target=\"_blank\">Apache Arrow</a>, an in-memory columnar data format that is used in Spark to efficiently transfer data between JVM and Python processes with near-zero (de)serialization cost\n",
    "* Pandas inside the function, to work with Pandas instances and APIs\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_warn_32.png\" alt=\"Warning\"> As of Spark 3.0, you should **always** define your Pandas UDF using Python type hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d558cbde-9c18-4360-b6d6-b1727c862e6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# We have a string input/output\n",
    "@pandas_udf(\"string\")\n",
    "def vectorized_udf(email: pd.Series) -> pd.Series:\n",
    "    return email.str[0]\n",
    "\n",
    "# Alternatively\n",
    "# def vectorized_udf(email: pd.Series) -> pd.Series:\n",
    "#     return email.str[0]\n",
    "# vectorized_udf = pandas_udf(vectorized_udf, \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07f45f5f-4ee1-4a5f-9fcd-335d6ff1f325",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(sales_df.select(vectorized_udf(col(\"email\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9542eb9-e790-4664-8644-def7724de574",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "We can also register these Pandas UDFs to the SQL namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82094732-b601-4d95-9a17-b4f8b9b1c439",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.udf.register(\"sql_vectorized_udf\", vectorized_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c09c99c6-8938-459e-a3e6-7fbb9eca0de7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Use the Pandas UDF from SQL\n",
    "SELECT sql_vectorized_udf(email) AS firstLetter FROM sales"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_03_Functions in Apache Spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
