{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acd2f3ce-9d78-4959-8691-b49908dc1c29",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2. Apache Spark Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3176d0a9-57b4-46fb-b3b3-afa7595524b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## SparkSQL\n",
    "- It is a **module** for Structured data processing with multiple interfaces\n",
    "- It includes any object that has a Schema or Structure, including SQL tables, DataFrames API for Python, Scala, Java and R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22503fe6-da15-487e-9095-161597f74ffd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Transformations\n",
    "- DataFrame Transformations are **lazily** eveluated (Job won't start until having **Action**)\n",
    "  - Schema eagerly evaludated by Driver, but Job not spawned\n",
    "  - Benefit of \"Lazy Evaluation\": Spark can make Optimization decisions after it look at the DAG (Directed Acyclic Graph)\n",
    "- Actions: are methods that trigger\n",
    "  - Job is spawned\n",
    "  - Examples: df.count(), df.collect(), df.show(), display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd01cd26-c554-499c-8b97-f72109dcdd3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## DataFrameReader\n",
    "- Interface used to load a DataFrame from external storage\n",
    "  - ```spark.read.csv(\"/Filestore/tables/LifeExp_headers.csv\")```\n",
    "- Explicit vs Implicit vs Infer Schema\n",
    "  1. **Explicitly** define Schema _**without reading**_ data files\n",
    "      ```\n",
    "      DDL_schema = (\"coutry STRING, lifeexp DOUBLE, region STRING)\n",
    "      userDF = spark.read.option(\"header\", True).schema(DDL_schema).csv(\"/Filestore/tables/LifeExp_headers.csv\")\n",
    "      ```\n",
    "  2. **Implicitly** create default Column names and Data types _**without reading**_ data files\n",
    "      ```\n",
    "      df1 = spark.read.load(\"/Filestore/tables/LifeExp_headers.csv\", format = \"csv\", header = False)\n",
    "      display(df1)\n",
    "      ```\n",
    "  3. **Infer** column names and data types _**by reading**_ data files\n",
    "      ```\n",
    "      df2 = spark.read.load(\"/Filestore/tables/LifeExp_headers.csv\", format = \"csv\", header = True, inferSchema = True)\n",
    "      display(df2)\n",
    "      ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd0d6f4d-534d-44bf-9b66-fcf710ea585e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## DataFrameWriter\n",
    "- Write DataFrame to external storage\n",
    "    ```\n",
    "    df.write\n",
    "      .format(\"delta)\n",
    "      .mode(\"append\")\n",
    "      .save(outPath)\n",
    "    ```\n",
    "- Write as SQL table\n",
    "    ```\n",
    "    df.write\n",
    "      .mode(\"overwrite\")\n",
    "      .saveAsTable(\"evants_p\")\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d66dc21-50d8-4104-8637-71ef96d7bb96",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Query Execution\n",
    "We can express the same query using any interface. The Spark SQL engine generates the same query plan used to optimize and execute on our Spark cluster.\n",
    "\n",
    "![query execution engine](https://files.training.databricks.com/images/aspwd/spark_sql_query_execution_engine.png)\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> Resilient Distributed Datasets (RDDs) are the low-level representation of datasets processed by a Spark cluster. In early versions of Spark, you had to write <a href=\"https://spark.apache.org/docs/latest/rdd-programming-guide.html\" target=\"_blank\">code manipulating RDDs directly</a>. In modern versions of Spark you should instead use the higher-level DataFrame APIs, which Spark automatically compiles into low-level RDD operations."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_02_Apache Spark Core",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
