{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0b39fa2-cf21-4add-9aea-c389fa295f4e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Prepare to use Apache Spark\n",
    "**Apache Spark:** is a **distributed data processing framework** that enables large-scale data analytics by coordinating work across multiple processing nodes in a cluster. Put more simply, Spark uses a \"divide and conquer\" approach to processing large volumes of data quickly by distributing the work across **multiple computers**.\n",
    "\n",
    "Spark can run code written in a **wide range of languages**, including Java, Scala (a Java-based scripting language), Spark R, Spark SQL, and PySpark (a Spark-specific variant of Python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7ddf0e1-3b4b-4baf-ad6b-31360ed4afb1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark settings\n",
    "In Microsoft Fabric, each workspace is assigned a **Spark cluster**. An administrator can manage settings for the Spark cluster in the Data Engineering/Science section of the workspace settings.\n",
    "\n",
    "<img src=\"./images/08/spark-settings.png\" alt=\"Spark settings\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "Specific configuration settings include:\n",
    "- **Node Family:** The type of **virtual machines** used for the Spark cluster nodes. In most cases, **memory optimized** nodes provide optimal performance.\n",
    "- **Runtime version:** The **version** of Spark (and dependent subcomponents) to be run on the cluster.\n",
    "- **Spark Properties:** Spark-specific **settings** that you want to enable or override in your cluster. You can see a list of properties in the [Apache Spark documentation](https://spark.apache.org/docs/latest/configuration.html#available-properties).\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> In most scenarios, the default settings provide an optimal configuration for Spark in Microsoft Fabric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acfb67e9-4854-45df-9d6b-f389ffaa7b56",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Libraries\n",
    "**The Spark open source ecosystem** includes a wide selection of **code libraries** for common (and sometimes very specialized) tasks. Since a great deal of Spark processing is performed using PySpark, the huge range of Python libraries ensures that whatever the task you need to perform, there's probably a library to help.\n",
    "\n",
    "By default, Spark clusters in Microsoft Fabric include many of the **most commonly used libraries**. In order to set additional default libraries or persist library specifications for code items, you need **`workspace admin permissions`** to create an environment and set the default environment for the workspace.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> For more information about library management, see Manage Apache Spark libraries in [Microsoft Fabric in the Microsoft Fabric documentation](https://learn.microsoft.com/en-us/fabric/data-engineering/library-management)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a7cbf5b-e0ef-4b61-a004-289d3c4aa826",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Run Spark code\n",
    "To edit and run Spark code in Microsoft Fabric, you can use **`notebooks`**, or you can define a **`Spark job`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e5b4b4f-4156-4170-b307-764215639055",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Notebooks\n",
    "**`Notebooks`** enable you to combine text, images, and code written in **multiple languages** to create an interactive item that you can share with others and collaborate.\n",
    "- Notebooks consist of one or more cells, each of which can contain markdown-formatted content or executable code. You can run the code interactively in the notebook and see the results immediately.\n",
    "\n",
    "<img src=\"./images/08/notebook.png\" alt=\"Notebook in Microsoft Fabric\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e9499f9-021b-4f35-8812-136dbd7a05e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark job definition\n",
    "If you want to use Spark to **ingest** and **transform** data as part of an automated process, you can define a **`Spark job`** to run a script on-demand or based on a schedule.\n",
    "- To configure a Spark job, create a *`Spark Job Definition`** in your workspace and specify the script it should run. \n",
    "- You can also specify a reference file (for example, a Python code file containing definitions of functions that are used in your script) and a reference to a specific lakehouse containing data that the script processes.\n",
    "\n",
    "<img src=\"./images/08/spark-job.png\" alt=\"Spark job definition in Microsoft Fabric\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8443666-e2c6-4fab-a945-587cbf34cbdb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Work with data in a Spark dataframe\n",
    "Natively, Spark uses a data structure called a **`resilient distributed dataset (RDD)`**; but while you can write code that works directly with RDDs, the most commonly used data structure for working with structured data in Spark is the **`dataframe`**, which is provided as part of the Spark SQL library. \n",
    "- Dataframes in Spark are similar to those in the ubiquitous Pandas Python library, but optimized to work in Spark's distributed processing environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "458b34ca-5401-4207-af2f-c5020fa2ea25",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Loading data into a dataframe\n",
    "Sample CSV file:\n",
    "\n",
    "```\n",
    "ProductID,ProductName,Category,ListPrice\n",
    "771,\"Mountain-100 Silver, 38\",Mountain Bikes,3399.9900\n",
    "772,\"Mountain-100 Silver, 42\",Mountain Bikes,3399.9900\n",
    "773,\"Mountain-100 Silver, 44\",Mountain Bikes,3399.9900\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6e67675-b010-40b2-9f0b-61595b43042a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Inferring a schema\n",
    "In a Spark notebook, you could use the following PySpark code to load the file data into a dataframe and display the first 10 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1ccde04-db50-4cb1-b692-56da4eae50b9",
     "showTitle": true,
     "title": "Language = PySpark"
    }
   },
   "outputs": [],
   "source": [
    "%%pyspark\n",
    "df = spark.read.load('Files/data/products.csv',\n",
    "    format='csv',\n",
    "    header=True\n",
    ")\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "905292ba-a28b-45a6-b21b-6709a19cc22a",
     "showTitle": true,
     "title": "Language = Scala"
    }
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "val df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"Files/data/products.csv\")\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30dd23d8-3af8-4115-b9b3-60fa2d30b462",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The both provide same output:\n",
    "\n",
    "<img src=\"./images/08/output_inferring_schema.png\" alt=\"Output Inferring Schema\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f4c38bb-5527-4c43-8729-32f38973a226",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Specifying an explicit schema\n",
    "You can also specify an explicit schema for the data, which is useful when the column names aren't included in the data file, like this CSV example:\n",
    "```\n",
    "771,\"Mountain-100 Silver, 38\",Mountain Bikes,3399.9900\n",
    "772,\"Mountain-100 Silver, 42\",Mountain Bikes,3399.9900\n",
    "773,\"Mountain-100 Silver, 44\",Mountain Bikes,3399.9900\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28dde1a3-6c63-4e69-8532-25b9d8cafd95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "productSchema = StructType([\n",
    "    StructField(\"ProductID\", IntegerType()),\n",
    "    StructField(\"ProductName\", StringType()),\n",
    "    StructField(\"Category\", StringType()),\n",
    "    StructField(\"ListPrice\", FloatType())\n",
    "    ])\n",
    "\n",
    "df = spark.read.load('Files/data/product-data.csv',\n",
    "    format='csv',\n",
    "    schema=productSchema,\n",
    "    header=False)\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f8dcf03-28b5-4e5b-93fa-868d20a4aa0b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The result would be the same\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> Specifying an explicit schema also **`improves performance`**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e464f7ce-fdc9-4fcd-87f9-1c6ea80bfb45",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Filtering and grouping dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cb19b90-f531-4b75-96ea-75b68ab9cff7",
     "showTitle": true,
     "title": "Select"
    }
   },
   "outputs": [],
   "source": [
    "## For example, the following code example uses the select method to retrieve the ProductID and ListPrice columns from the df dataframe containing product data in the previous example:\n",
    "pricelist_df = df.select(\"ProductID\", \"ListPrice\")\n",
    "\n",
    "#Selecting a subset of columns from a dataframe is a common operation, which can also be achieved by using the following shorter syntax:\n",
    "pricelist_df = df[\"ProductID\", \"ListPrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b797524-0811-483b-b4ba-51110313e0a0",
     "showTitle": true,
     "title": "Where"
    }
   },
   "outputs": [],
   "source": [
    "#For example, this example code chains the select and where methods to create a new dataframe containing the ProductName and ListPrice columns for products with a category of Mountain Bikes or Road Bikes:\n",
    "bikes_df = df.select(\"ProductName\", \"Category\", \"ListPrice\").where((df[\"Category\"]==\"Mountain Bikes\") | (df[\"Category\"]==\"Road Bikes\"))\n",
    "display(bikes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d90fea6-4037-43e1-9dd1-33a3a4ae4a29",
     "showTitle": true,
     "title": "Group & Aggregate"
    }
   },
   "outputs": [],
   "source": [
    "#For example, the following PySpark code counts the number of products for each category:\n",
    "counts_df = df.select(\"ProductID\", \"Category\").groupBy(\"Category\").count()\n",
    "display(counts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ca55b05-7b94-48f3-8adc-b66d79f9f141",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Saving a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6baba047-2d0f-41b3-b2b3-c71e4b10471e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The following code example saves the dataFrame into a parquet file in the data lake, replacing any existing file of the same name.\n",
    "bikes_df.write.mode(\"overwrite\").parquet('Files/product_data/bikes.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c444855-fd79-4776-a3f0-8b7098b893fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Partitioning the output file\n",
    "**Partitioning:** is an optimization technique that enables Spark to **maximize performance** across the worker nodes. More performance gains can be achieved when filtering data in queries by eliminating unnecessary disk IO.\n",
    "\n",
    "To save a dataframe as a partitioned set of files, use the **`partitionBy`** method when writing the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3613a95e-8be2-4a81-83cf-02f50f678234",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#The following example saves the bikes_df dataframe (which contains the product data for the mountain bikes and road bikes categories), and partitions the data by category:\n",
    "bikes_df.write.partitionBy(\"Category\").mode(\"overwrite\").parquet(\"Files/bike_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41348dc0-8680-43d1-9d20-09554aee13a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The folder names generated when partitioning a dataframe include the partitioning column name and value in a **`column=value format`**, so the code example creates a folder named **`bike_data`** that contains the following **subfolders**:\n",
    "- Category=Mountain Bikes\n",
    "- Category=Road Bikes\n",
    "\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> You can partition the data by **`multiple columns`**, which results in a **`hierarchy of folders`** for each partitioning key. For example, you might partition sales order data by year and month, so that the folder hierarchy includes a folder for each year value, which in turn contains a subfolder for each month value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efb10588-dae4-4075-8713-ba939f27b69f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load partitioned data\n",
    "When reading partitioned data into a dataframe, you can load data from any folder within the hierarchy by specifying explicit values or wildcards for the partitioned fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c3bd5ab-786f-4fb4-9fd6-c9ee5f929f35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# When reading partitioned data into a dataframe, you can load data from any folder within the hierarchy by specifying explicit values or wildcards for the partitioned fields. The following example loads data for products in the Road Bikes category:\n",
    "road_bikes_df = spark.read.parquet('Files/bike_data/Category=Road Bikes')\n",
    "display(road_bikes_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dec8f108-ee07-439f-9873-358733fd661b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> The **partitioning columns** specified in the file path are **`omitted`** in the resulting dataframe. The results produced by the example query would not include a Category column - the category for all rows would be Road Bikes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc670f79-877f-44eb-bf06-f33e4f23629d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Work with data using Spark SQL\n",
    "**`Spark SQL`** enables data analysts to use **SQL expressions** to query and manipulate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84fe2bbf-da65-4e58-95bb-349951626782",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Creating database objects in the Spark catalog\n",
    "**The Spark catalog:** is a **`metastore`** for **relational data objects** such as views and tables. \n",
    "- **The Spark runtime** can use the **catalog** to seamlessly integrate code written in any Spark-supported language with SQL expressions that may be more natural to some data analysts or developers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9820e31-38e3-4b22-a43f-d1b7a6da666e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Temporary View\n",
    "One of the simplest ways to make data in a dataframe available for querying in the Spark catalog is to create a **`temporary view`**\n",
    "- A view is **temporary**, meaning that it's automatically deleted at the end of the current session, as shown in the following code example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4577bc3c-131c-4516-b63c-50737066b1d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"products_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a137cf77-1ec5-47e0-b30c-7a4b6b39704d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Tables\n",
    "**`Tables`** are **metadata structures** that store their underlying data in the **storage location** associated with the catalog.\n",
    "- Tables are **persisted in the catalog** to define a database that can be queried using Spark SQL.\n",
    "- In Microsoft Fabric, data for managed tables is stored in the Tables storage location shown in **`data lake`**, and any tables created using Spark are listed there.\n",
    "\n",
    "You can create an empty table by using the `spark.catalog.createTable` method, or you can save a dataframe as a table by using its `saveAsTable` method. Deleting a managed table also deletes its underlying data.\n",
    "\n",
    "For example, the following code saves a dataframe as a new table named products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e036448-a3ef-4bd5-bb59-b10735d69fd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").saveAsTable(\"products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e76efe93-216a-4f58-a64a-e1ff117e29a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> The Spark catalog supports tables based on files in various formats. The preferred format in Microsoft Fabric is **`delta`**, which is the format for a relational data technology on Spark named Delta Lake. Delta tables support features commonly found in relational database systems, including transactions, versioning, and support for streaming data.\n",
    "\n",
    "Additionally, you can create external tables by using the `spark.catalog.createExternalTable` method. External tables define metadata in the catalog but get their underlying data from an **external storage location**; typically a folder in the Files storage area of a lakehouse. Deleting an external table doesn't delete the underlying data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0dbbee7-ffef-4bdc-a5f0-f7b3316f26c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Using the Spark SQL API to query data\n",
    "You can use the **`Spark SQL API`** in code written in any language to query data in the catalog. \n",
    "\n",
    "For example, the following PySpark code uses a SQL query to return data from the products table as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "894305c0-8e64-4cf8-91af-6fdf772d9a67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bikes_df = spark.sql(\"SELECT ProductID, ProductName, ListPrice \\\n",
    "                      FROM products \\\n",
    "                      WHERE Category IN ('Mountain Bikes', 'Road Bikes')\")\n",
    "display(bikes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "700eeb36-14c7-4267-b27d-e0f323f33376",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Using SQL code\n",
    "In a notebook, you can also use the %%sql magic to run **`SQL code`** that queries objects in the catalog, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bb32815-243a-4017-87c6-23070cf2de53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT Category, COUNT(ProductID) AS ProductCount\n",
    "FROM products\n",
    "GROUP BY Category\n",
    "ORDER BY Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf69be95-3af8-468f-8e56-0ba202f711d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Visualize data in a Spark notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bc8e645-529e-427a-bdb1-76ccac6c9b96",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Using built-in notebook charts\n",
    "By default, results are rendered as a table, but you can also change the results view to a chart and use the **`chart properties`** to **customize** how the chart visualizes the data, as shown here:\n",
    "\n",
    "<img src=\"./images/08/notebook-chart.png\" alt=\"Notebook chart\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "793191dc-6079-40c2-8672-06c041434153",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Using graphics packages in code\n",
    "There are many **graphics packages** that you can use to create data visualizations in code. In particular, Python supports a large selection of packages; most of them built on the base Matplotlib library. \n",
    "- The output from a graphics library can be rendered in a notebook, making it easy to combine code to ingest and manipulate data with inline data visualizations and markdown cells to provide commentary.\n",
    "\n",
    "For example, you could use the following PySpark code to aggregate data from the hypothetical products data explored previously in this module, and use Matplotlib to create a chart from the aggregated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa95b316-1d2a-4ccb-9906-10085b1b3312",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Get the data as a Pandas dataframe\n",
    "data = spark.sql(\"SELECT Category, COUNT(ProductID) AS ProductCount \\\n",
    "                  FROM products \\\n",
    "                  GROUP BY Category \\\n",
    "                  ORDER BY Category\").toPandas()\n",
    "\n",
    "# Clear the plot area\n",
    "plt.clf()\n",
    "\n",
    "# Create a Figure\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "\n",
    "# Create a bar plot of product counts by category\n",
    "plt.bar(x=data['Category'], height=data['ProductCount'], color='orange')\n",
    "\n",
    "# Customize the chart\n",
    "plt.title('Product Counts by Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Products')\n",
    "plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)\n",
    "plt.xticks(rotation=70)\n",
    "\n",
    "# Show the plot area\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4f985ae-e038-4b19-9361-8ffc76a27aa4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The Matplotlib library requires data to be in a Pandas dataframe rather than a Spark dataframe, so the toPandas method is used to convert it. The code then creates a figure with a specified size and plots a bar chart with some custom property configuration before showing the resulting plot.\n",
    "\n",
    "The chart produced by the code would look similar to the following image:\n",
    "\n",
    "<img src=\"./images/08/chart.png\" alt=\"Chart\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "08_Use Apache Spark in Microsoft Fabric",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
