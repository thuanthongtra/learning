{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0b39fa2-cf21-4add-9aea-c389fa295f4e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Explore data load strategies\n",
    "In Microsoft Fabric, there are many ways you can choose to load data in a warehouse. \n",
    "- This step is fundamental as it ensures that high-quality, transformed or processed data is integrated into a **single repository**.\n",
    "- Also, the efficiency of data loading directly impacts the **timeliness** and **accuracy** of analytics, making it vital for real-time decision-making processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68bda68b-62bb-45c9-a294-65e877229875",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Understand data ingestion and data load operations\n",
    "While both processes are part of the ETL (Extract, Transform, Load) pipeline in a data warehouse scenario, they usually serve different purposes. \n",
    "- **Data ingestion/extract:** is about **`moving`** raw data from various sources into a central repository. \n",
    "- **Data loading:** involves **`taking`** the **transformed** or **processed** data and **`loading`** it into the final storage destination for analysis and reporting.\n",
    "\n",
    "All Fabric data items like data warehouses and lakehouses store their data automatically in OneLake in Delta Parquet format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c6ab9b7-388b-450e-bee5-b218041f54b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Stage your data\n",
    "You may have to build and work with **auxiliary objects** involved in a **load operation** such as tables, stored procedures, and functions. These auxiliary objects are commonly referred to as **staging**.\n",
    "- Staging objects act as temporary storage and transformation areas.\n",
    "- They can share resources with a data warehouse, or live in its own storage area.\n",
    "\n",
    "Staging serves as an abstraction layer, simplifying and facilitating the load operation to the final tables in the data warehouse.\n",
    "\n",
    "<img src=\"../images/04_Implement a data warehouse with Microsoft Fabric/01/1-data-warehouse-process.png\" alt=\"Sequential steps\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "Also, staging area provides a buffer that can help to minimize the impact of the load operation on the performance of the data warehouse. This is important in environments where the data warehouse needs to remain operational and responsive during the data loading process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5301eef1-8807-4800-ab7b-e4516500821e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Review type of data loads\n",
    "There are two types of data loads to consider when loading a data warehouse.\n",
    "\n",
    "\n",
    "| Load Type\t| Description\t| Operation\t| Duration\t| Complexity\t| Best used\t|\n",
    "| ----------|-------------|-----------|-----------|-------------|-----------|\n",
    "| Full (initial) load | The process of populating the data warehouse for the **first time**. | All the tables are **truncated** and **reloaded**, and the old data is lost | It may take longer to complete due to the amount of data being handled | Easier to implement as there's no history preserved | This method is typically used when setting up a **new data warehouse**, or when a **complete refresh of the data** is required |\n",
    "| Incremental load | The process of **updating** the data warehouse with the changes since the last update | The history is preserved, and tables are **updated** with new information | Takes less time than the initial load\tImplementation is more complex than the initial load | This method is commonly used for regular updates to the data warehouse, such as daily or hourly updates. | It requires mechanisms to **track changes** in the source data since the last load. |\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> To learn more about how to perform an incremental load, see [Incremental load](https://learn.microsoft.com/en-us/fabric/data-factory/tutorial-incremental-copy-data-warehouse-lakehouse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18fdedf0-9e10-43b8-b1d8-d1539cc7fdd4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load a dimension table\n",
    "Think of a dimension table as the \"who, what, where, when, why” of your data warehouse. It’s like the **descriptive backdrop** that gives context to the raw numbers found in the fact tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6d030b1-db29-4288-8f68-4cdfd344feb8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Slowly changing dimensions (SCD)\n",
    "**Slowly Changing Dimensions:** change over time, but at a **slow pace** and **unpredictably**. There are several types of SCD in a data warehouse, with type 1 and type 2 being the most frequently used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c1fd09b-3847-4acc-996c-92673ce40964",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### SCD 0 - Retain Original\n",
    "- The dimension attributes **`never change`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "474b2966-3cf0-429e-84e4-51d3e19650a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### SCD 1 - Overwrite\n",
    "- **`Overwrites`** existing data, doesn't keep history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3667339-0190-4a2b-b292-6b66a5b5fc5c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### SCD 2 - Add New Row\n",
    "- **Adds `new records`** for changes, keeps full history for a given natural key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d0beb5b-aba4-4e43-a086-d8a6230f42ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### SCD 3 - Add New Attribute\n",
    "- History is **added as a `current` column** to record **current** and **previous** values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4762de69-9015-4fd9-a83b-325a136d7e96",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### SCD 4 - Add Mini-Dimension\n",
    "- A **new `mini-dimension` is added**.\n",
    "- The type 4 technique is used when:\n",
    "  - A **`group of attributes`** in a dimension **rapidly changes** and is **split off** to a mini-dimension. This situation is sometimes called a **_rapidly changing monster dimension_**. \n",
    "  - **Frequently `used` attributes** in multimillion-row dimension tables are mini-dimension design **candidates**, even if they don’t frequently change. \n",
    "- The surrogate keys of both the base dimension and mini-dimension are captured in the associated fact tables.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "<img src=\"../images/04_Implement a data warehouse with Microsoft Fabric/01/SCD_4.png\" alt=\"SCD 4\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28920ed8-10ea-44c4-835c-809bc3f14078",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### SCD 5 - Add Mini-Dimension and Type 1 Outtrigger\n",
    "- The type 5 technique is used to:\n",
    "  - Accurately preserve **`historical` attribute values**, plus \n",
    "  - Report historical facts according to **`current` attribute values**. \n",
    "- **Type 5** builds on the **`type 4` mini-dimension** by also embedding a current **`type 1`** reference to the mini-dimension in the base dimension. \n",
    "  - This enables the currently-assigned minidimension attributes to be accessed along with the others in the base dimension without linking through a fact table. \n",
    "  - Logically, you’d represent the base dimension and mini-dimension outrigger as a single table in the presentation area. \n",
    "- The ETL team **must `overwrite`** this type 1 mini-dimension reference whenever the current mini-dimension assignment changes.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "<img src=\"../images/04_Implement a data warehouse with Microsoft Fabric/01/SCD_5.png\" alt=\"SCD 5\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2eb7d958-1093-4cfa-a75b-85578339119f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### SCD 6 - Add Type 1 Attribute To Type 2 Dimension\n",
    "- **Type 6** builds on:\n",
    "  - The **`type 2`** technique by \n",
    "  - also embedding current **`type 1`** versions of the same attributes in the dimension row so that fact rows can be filtered or grouped by either the type 2 attribute value in effect when the measurement occurred or the attribute’s current value. \n",
    "- In this case, the type 1 attribute is systematically **`overwritten` on all rows** associated with a particular durable key whenever the attribute is updated.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "<img src=\"../images/04_Implement a data warehouse with Microsoft Fabric/01/SCD_6.png\" alt=\"SCD 6\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51a00abe-c4ff-407e-bad7-4045447f2c30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- The following example shows how to handle changes in a type 2 SCD for the Dim_Products table using T-SQL.\n",
    "IF EXISTS (SELECT 1 FROM Dim_Products WHERE SourceKey = @ProductID AND IsActive = 'True')\n",
    "BEGIN\n",
    "    -- Existing product record\n",
    "    UPDATE Dim_Products\n",
    "    SET ValidTo = GETDATE(), IsActive = 'False'\n",
    "    WHERE SourceKey = @ProductID AND IsActive = 'True';\n",
    "END\n",
    "ELSE\n",
    "BEGIN\n",
    "    -- New product record\n",
    "    INSERT INTO Dim_Products (SourceKey, ProductName, StartDate, EndDate, IsActive)\n",
    "    VALUES (@ProductID, @ProductName, GETDATE(), '9999-12-31', 'True');\n",
    "END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "883f9f96-d8d1-4439-8b3b-516f9ed9701c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The mechanism for detecting changes in source systems is crucial for determining when records are inserted, updated, or deleted. [Change Data Capture (CDC)](https://learn.microsoft.com/en-us/sql/relational-databases/track-changes/about-change-data-capture-sql-server?view=sql-server-ver16), [change tracking](https://learn.microsoft.com/en-us/sql/relational-databases/track-changes/about-change-tracking-sql-server?view=sql-server-ver16), and [triggers](https://learn.microsoft.com/en-us/sql/relational-databases/triggers/dml-triggers?view=sql-server-ver16) are all features available for managing data tracking in source systems such as SQL Server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0712ea4f-e3be-4920-b62c-840c3009b66b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load a fact table\n",
    "Let's consider an example where we load a Fact_Sales table in a data warehouse. This table contains sales transactions data with columns such as FactKey, DateKey, ProductKey, OrderID, Quantity, Price, and LoadTime.\n",
    "\n",
    "Assume we have a source table Order_Detail in an OLTP system with columns: OrderID, OrderDate, ProductID, Quantity, and Price.\n",
    "\n",
    "The following T-SQL script example load the Fact_Sales table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f3f7198-e91c-40d3-8d3e-32eba6241099",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Lookup keys in dimension tables\n",
    "INSERT INTO Fact_Sales (DateKey, ProductKey, OrderID, Quantity, Price, LoadTime)\n",
    "SELECT d.DateKey, p.ProductKey, o.OrderID, o.Quantity, o.Price, GETDATE()\n",
    "FROM Order_Detail o\n",
    "JOIN Dim_Date d ON o.OrderDate = d.Date\n",
    "JOIN Dim_Product p ON o.ProductID = p.ProductID;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c3eccb1-f5ee-46d9-878b-41ad4082fa65",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this example, we use a JOIN operation to look up the **DateKey** and **ProductKey** values in the **Dim_Date** and **Dim_Product** tables, respectively, and then insert the data into the Fact_Sales table. \n",
    "- However, it is important to note that the complexity of the loading process depends on several factors, including the amount of data, the transformation requirements, error handling, schema differences, and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e47e1e4b-bbfe-450c-bea9-db1489893cc2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Use data pipelines to load a warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e1ea19e-e72a-4405-88cd-fc1eb34d2f29",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create a data pipeline\n",
    "There are a few ways to launch the data pipeline editor.\n",
    "\n",
    "- **From the workspace:** Select + **New**, then select **Data pipeline**. If it's not visible in the list, select **More options**, then find **Data pipeline** under the **Data Factory section**.\n",
    "\n",
    "<img src=\"../images/04_Implement a data warehouse with Microsoft Fabric/01/3-data-pipeline-create.gif\" alt=\"Launch Data Pipeline from the workspace\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "- **From the warehouse asset:** Select **Get Data**, and then **New data pipeline**.\n",
    "\n",
    "<img src=\"../images/04_Implement a data warehouse with Microsoft Fabric/01/3-create-data-pipeline.png\" alt=\"Shortcuts for a few features in the Warehouse asset\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74073e7c-ba81-46f4-bed6-dec17cc4cf01",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are three options available when creating a pipeline.\n",
    "\n",
    "<img src=\"../images/04_Implement a data warehouse with Microsoft Fabric/01/3-build-pipeline.png\" alt=\"Options available when creating a pipeline\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "| Option | Description |\n",
    "| -- | -- |\n",
    "| 1. Add pipeline activity | Launches the pipeline editor where you can create your own pipeline. |\n",
    "| 2. Copy data | Launches an assistant to copy data from various data sources to a data destination. A new pipeline activity is generated at the end with a preconfigured Copy Data task. |\n",
    "| 3. Choose a task to start | You can choose from a collection of predefined templates to assist you in initiating pipelines based on many scenarios. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39be6b7b-3d64-4aa8-a500-71bfafcb6021",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Configure the copy data assistant\n",
    "The copy data assistant provides a step-by-step interface that facilitates the configuration of a **Copy Data** task.\n",
    "\n",
    "<img src=\"../images/04_Implement a data warehouse with Microsoft Fabric/01/3-copy-data-assistant.png\" alt=\"Copy data assistant\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "- **Choose data source:** Select a connector, and provide the connection information.\n",
    "- **Connect to a data source:** Select, preview, and choose the data. This can be done from tables or views, or you can customize your selection by providing your own query.\n",
    "- **Choose data destination:** Select the data store as the destination.\n",
    "- **Connect to data destination:** Select and map columns from source to destination. You can load to a new or existing table.\n",
    "- **Settings:** Configure other settings like staging, and default values.\n",
    "\n",
    "After you copy the data, you can use other tasks to further transform and analyze it. You can also use the **Copy Data** task to publish transformation and analysis results for business intelligence (BI) and application consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "168c33df-40b9-4afe-a933-0fbcae35eb24",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Schedule a data pipeline\n",
    "You can schedule your data pipeline by selecting **Schedule** from the data pipeline editor.\n",
    "\n",
    "<img src=\"../images/04_Implement a data warehouse with Microsoft Fabric/01/3-schedule-data-pipeline.png\" alt=\"Schedule a data pipeline from the pipeline designer\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "You can also configure the schedule by selecting **Settings** in the **Home** menu in the data pipeline editor.\n",
    "\n",
    "<img src=\"../images/04_Implement a data warehouse with Microsoft Fabric/01/3-schedule-configuration.png\" alt=\"Configuration properties when you schedule a data pipeline\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "We recommend data pipelines for a code-free or low-code experience due to the graphical user interface. They're ideal for data workflows that run at a schedule, or that connects to different data sources.\n",
    "\n",
    "To learn more about data pipelines, see [Ingest data into your Warehouse using data pipelines](https://learn.microsoft.com/en-us/fabric/data-warehouse/ingest-data-pipelines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a7989b9-e93f-4339-8f7c-f6a1dc7ce4b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Load data using T-SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1889329-0024-4454-b26a-798ae8d50e88",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Use COPY statement\n",
    "The [COPY statement](https://learn.microsoft.com/en-us/sql/t-sql/statements/copy-into-transact-sql?view=azure-sqldw-latest) serves as the main method for importing data into the Warehouse. It facilitates efficient data ingestion from an external Azure storage account.\n",
    "\n",
    "It offers flexibility, allowing you to specify the format of the source file, designate a location for storing rows that are rejected during the import process, skip header rows, among other configurable options.\n",
    "\n",
    "The option to store rejected rows separately is useful for data cleaning and quality control. It allows you to easily identify and investigate any issues with the data that weren't successfully imported.\n",
    "\n",
    "To connect to an Azure storage account, you need to use either Shared Access Signature (SAS) or Storage Account Key (SAK).\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> The COPY statement currently supports the PARQUET and CSV file formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4060b74-40b4-4097-a4b9-a9eb7fe847f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Handle error\n",
    "The option to use a different storage account for the _ERRORFILE_ location (`REJECTED_ROW_LOCATION`) allows for better error handling and debugging. It makes it easier to isolate and investigate any issues that occur during the data loading process. _ERRORFILE_ only applies to CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9b380b4-04fb-41fd-af0f-3cede94d1d4f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load multiple files\n",
    "The ability to specify wildcards and multiple files in the storage location path allows the COPY statement to handle bulk data loading efficiently. This is useful when dealing with large datasets distributed across multiple files.\n",
    "\n",
    "Multiple file locations can only be specified from the same storage account and container via a comma-separated list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84df55bf-aba6-4796-b84a-6c1e58c6b4ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "COPY my_table\n",
    "FROM 'https://myaccount.blob.core.windows.net/myblobcontainer/folder0/*.csv, \n",
    "    https://myaccount.blob.core.windows.net/myblobcontainer/folder1/'\n",
    "WITH (\n",
    "    FILE_TYPE = 'CSV',\n",
    "    CREDENTIAL=(IDENTITY= 'Shared Access Signature', SECRET='<Your_SAS_Token>')\n",
    "    FIELDTERMINATOR = '|'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94c0a637-8571-4f48-a5f1-d53a841d3fec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--The following example shows how to load a PARQUET file.\n",
    "COPY INTO test_parquet\n",
    "FROM 'https://myaccount.blob.core.windows.net/myblobcontainer/folder1/*.parquet'\n",
    "WITH (\n",
    "    CREDENTIAL=(IDENTITY= 'Shared Access Signature', SECRET='<Your_SAS_Token>')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1f6cf7f-6709-447a-a5cf-6b2e9cf74727",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load table from other warehouses and lakehouses\n",
    "You can load data from various data assets in a workspace, such as other warehouses and lakehouses.\n",
    "\n",
    "To reference the data asset, ensure that you use [three-part naming](https://learn.microsoft.com/en-us/sql/t-sql/language-elements/transact-sql-syntax-conventions-transact-sql?view=sql-server-ver16) to combine data from tables on these workspace assets. You can then use `CREATE TABLE AS SELECT` (CTAS) and `INSERT...SELECT` to load the data into the warehouse.\n",
    "\n",
    "| SQL Statement\t| Description |\n",
    "|--|--|\n",
    "| [CREATE TABLE AS SELECT](https://learn.microsoft.com/en-us/sql/t-sql/statements/create-table-as-select-azure-sql-data-warehouse?view=azure-sqldw-latest)\t| Allows you to create a new table based on the output of a `SELECT` statement. This operation is often used for creating a copy of a table or for transforming and loading the results of complex queries. |\n",
    "| [INSERT...SELECT](https://learn.microsoft.com/en-us/sql/t-sql/statements/insert-transact-sql?view=sql-server-ver16)\t| Allows you to insert data from one table into another. It’s useful when you want to copy data from one |\n",
    "\n",
    "In a scenario where an analyst needs data from both a warehouse and a lakehouse, they can use this feature to combine the data. They can then load this combined data into the warehouse for analysis. This feature is useful when data is distributed across many assets in a workspace.\n",
    "\n",
    "The following query creates a new table in the `analysis_warehouse` that combines data from the `sales_warehouse` and the `social_lakehouse` using the _product_id_ as the common key. The new table can then be used for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e30eae7-59f2-455f-8f32-1e2836310eab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE [analysis_warehouse].[dbo].[combined_data]\n",
    "AS\n",
    "SELECT \n",
    "FROM [sales_warehouse].[dbo].[sales_data] sales\n",
    "INNER JOIN [social_lakehouse].[dbo].[social_data] social\n",
    "ON sales.[product_id] = social.[product_id];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "269ebeab-b856-4d38-adf2-068cd473bed2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "All the Warehouses that share the same workspace are integrated into the same logical SQL server. If you use SQL client tools such as [SQL Server Management Studio](https://learn.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-ver16), you can easily perform a cross-database query like in any SQL Server instance.\n",
    "\n",
    "<img src=\"../images/04_Implement a data warehouse with Microsoft Fabric/01/4-load-using-ssms.gif\" alt=\"Reference other Warehouses in a workspace from SQL Server Management Studio\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "_MyWarehouse_ and _Sales_ are both warehouse assets that share the same workspace.\n",
    "\n",
    "If you’re using the object Explorer from the workspace to query your Warehouses, you need to add them explicitly. The warehouses added will also be visible from the Visual query editor.\n",
    "\n",
    "<img src=\"../images/04_Implement a data warehouse with Microsoft Fabric/01/4-query-using-workspace.gif\" alt=\"Query other Warehouses in a workspace from the Fabric workspace\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "When using T-SQL, data can be efficiently loaded into a warehouse in Microsoft Fabric through the COPY statement, or from other warehouses and lakehouses within the same workspace, allowing for seamless data management and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e69b0d4-fb21-4425-b85a-96757df0ec1f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Load and transform data with Dataflow Gen2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a24a153b-6f6f-4165-b1ae-eaa43be8cdbe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create a dataflow\n",
    "To create a new dataflow, navigate to your workspace, then select + **New**. If **Dataflow Gen2** isn't visible in the list, select **More options**, then find **Dataflow Gen2** under the **Data Factory section**.\n",
    "\n",
    "<img src=\"../images/04_Implement a data warehouse with Microsoft Fabric/01/5-load-using-dataflow.gif\" alt=\"Launch Dataflow Gen2 from the workspace\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20064b5f-9e15-4e6b-90a4-be4c009e592e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Import data\n",
    "Once the Dataflow Gen2 launches, there are many options to load your data available.\n",
    "\n",
    "<img src=\"../images/04_Implement a data warehouse with Microsoft Fabric/01/5-import-options.png\" alt=\"Launch Data Pipeline from the Warehouse asset\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "You can load different file types with just a few steps. For example, to load a text or CSV file from your local computer.\n",
    "\n",
    "<img src=\"../images/04_Implement a data warehouse with Microsoft Fabric/01/5-load-file.png\" alt=\"Load a text or CSV file\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "Once the data is imported you can start authoring your dataflow, you might decide to clean your data, reshape, remove columns, and create new ones. All the steps you perform are saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59b3d8c5-0190-4137-a3ec-74af7ef508da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Transform data with Copilot\n",
    "Copilot can be a valuable tool for assisting with dataflow transformations. Let's say we have a _Gender_ column that contains '_Male_' and '_Female_' and we want to transform it.\n",
    "\n",
    "The first step is to activate Copilot within your dataflow. Once that's done, you can then provide specific instructions on the transformation you want to perform.\n",
    "\n",
    "For instance, you might input the following command: \"_Transform the Gender column. If Male 0, if Female 1. Then convert it to integer_.\"\n",
    "\n",
    "<img src=\"../images/04_Implement a data warehouse with Microsoft Fabric/01/5-copilot.png\" alt=\"Use Copilot to apply transformation in a dataflow\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "Copilot adds a new step automatically, and you can always revert it if you want, or continue to build on it for further transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dd779ae-dc43-47b1-9976-11b7052c6f81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Add a data destination\n",
    "With the **Add data destination** feature, you can separate your ETL logic and destination storage. This separation can lead to cleaner, more maintainable code and can make it easier to modify either the ETL process or the storage configuration without affecting the other.\n",
    "\n",
    "Once the data is transformed, the next step is to add a destination step. On the **Query settings** tab, select + to add a destination step in your dataflow.\n",
    "\n",
    "<img src=\"../images/04_Implement a data warehouse with Microsoft Fabric/01/5-add-destination.png\" alt=\"Option to add a data destination in a dataflow\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "The following destination options are available.\n",
    "- Azure SQL Database\n",
    "- Lakehouse\n",
    "- Azure Data Explorer (Kusto)\n",
    "- Azure Synapse Analytics (SQL DW)\n",
    "- Warehouse\n",
    "\n",
    "Data that’s loaded into a destination like a warehouse can be easily accessed and analyzed using various tools. This improves the accessibility of your data and allows for more flexible and comprehensive data analysis.\n",
    "\n",
    "When you select a warehouse as a destination, you can choose the following update methods.\n",
    "\n",
    "Diagram showing visually the difference between the append and replace methods to update a row.\n",
    "\n",
    "<img src=\"../images/04_Implement a data warehouse with Microsoft Fabric/01/5-update-table-options.png\" alt=\"Difference between the append and replace methods to update a row\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "- **Append:** Add new rows to an existing table.\n",
    "- **Replace:** Replace the entire content of a table with a new set of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed2e8416-c328-40c4-9a8e-2ebdfe79e7f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Publish a dataflow\n",
    "After you choose your update method, the final step is to publish your dataflow.\n",
    "\n",
    "Publishing makes your transformations and data loading operations live, allowing the dataflow to be executed either manually or on a schedule. This process encapsulates your ETL operations into a single and reusable unit, streamlining your data management workflow.\n",
    "\n",
    "Any changes made in the dataflow take effect when it’s published. So, always ensure to publish your dataflow after making any relevant modifications."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Load data into a Microsoft Fabric data warehouse",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
