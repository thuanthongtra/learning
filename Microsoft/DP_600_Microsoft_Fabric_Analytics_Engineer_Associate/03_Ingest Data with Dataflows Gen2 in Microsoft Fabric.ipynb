{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05978be5-a1d2-4f88-8f07-9d4fed7bb564",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Understand Dataflows Gen2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0acf712-1a1e-41b9-b4f6-77cad0d97527",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## What is a dataflow?\n",
    "**Dataflows Gen2:** allow you to extract data from various sources, transform it using a wide range of transformation operations, and load it into a destination. \n",
    "  - Using **`Power Query Online`** also allows for a visual interface to perform these tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80ab1955-b5b1-4905-a0f0-e1387d3cc720",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## How to use Dataflows Gen2\n",
    "The goal of Dataflows Gen2 is to provide an easy, reusable way to perform ETL tasks using **Power Query Online**.\n",
    "\n",
    "Dataflows allow you to promote reusable ETL logic that prevents the need to create more connections to your data source. Dataflows offer a wide variety of transformations, and can be run manually, on a refresh schedule, or as part of a Data Pipeline orchestration.\n",
    "\n",
    "Dataflows can be **horizontally partitioned** as well.\n",
    "  - Once you create a global dataflow, data analysts can use dataflows to create specialized semantic models for specific needs.\n",
    "\n",
    "**Ways 1 - `Data Pipeline`:** you copy data, then use your preferred coding language to **`extract`**, **`transform`**, and **`load`** the data. \n",
    "\n",
    "**Ways 2 - `Dataflow Gen2`:** Alternatively, you can create a **Dataflow Gen2** first to **`extract`** and **`transform`** the data. \n",
    "- You can also **`load`** the data into a Lakehouse, and other destinations. Adding a data destination to your dataflow is optional, and the dataflow preserves all transformation steps. \n",
    "- To perform other tasks or load data to a different destination after transformation, create a **Data Pipeline** and add the **Dataflow Gen2** activity to your orchestration.\n",
    "\n",
    "**Way 3 - `Data Pipeline`** + **`Dataflow Gen2`**: for ELT (Extract, Load, Transform) process. \n",
    "- For this order, you'd use a **Data Pipeline** to **`extract`** and **`load`** the data into your preferred destination, such as the Lakehouse. \n",
    "- Then you'd create a **Dataflow Gen2** to connect to Lakehouse data to cleanse and **`transform`** data. In this case, you'd offer the Dataflow as a curated semantic model for data analysts to develop reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16cdeab5-2e97-4c4d-af7b-acb53d5e608f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Benefits and limitations\n",
    "Benefits:\n",
    "- Extend data with consistent data, such as a standard date dimension table.\n",
    "- Allow self-service users access to a subset of data warehouse separately.\n",
    "- Optimize performance with dataflows, which enable extracting data once for reuse, reducing data refresh time for slower sources.\n",
    "- Simplify data source complexity by only exposing dataflows to larger analyst groups.\n",
    "- Ensure consistency and quality of data by enabling users to clean and transform data before loading it to a destination.\n",
    "- Simplify data integration by providing a low-code interface that ingests data from various sources.\n",
    "\n",
    "Limitations:\n",
    "- Not a replacement for a data warehouse.\n",
    "- Row-level security isn't supported.\n",
    "- Fabric capacity workspace is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0097c4f-18d4-4bb2-8a7a-0c9c65aee9be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Explore Dataflows Gen2\n",
    "In Microsoft Fabric, you can create a Dataflow Gen2 in the Data Factory workload or Power BI workspace, or directly in the lakehouse.\n",
    "\n",
    "<img src=\"./images/03/power-query-online-overview.png\" alt=\"Power Query Online\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e4bc9e4-3be4-4ee7-be4c-d03f17d3272c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (1) Power Query ribbon\n",
    "**Dataflows Gen2** support a wide variety of data **source connectors** and numerous data **transformations** possible, such as:\n",
    "- Filter and Sort rows\n",
    "- Pivot and Unpivot\n",
    "- Merge and Append queries\n",
    "- Split and Conditional split\n",
    "- Replace values and Remove duplicates\n",
    "- Add, Rename, Reorder, or Delete columns\n",
    "- Rank and Percentage calculator\n",
    "- Top N and Bottom N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5259516-5b7c-419e-acf1-f4881c53270c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (2) Queries pane\n",
    "**The Queries pane** shows you the different data sources - now called queries. \n",
    "  - Rename, duplicate, reference, and enable staging are some of the options available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c3062e5-5227-4bd6-9a38-51d61c528750",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (3) Diagram view\n",
    "**The Diagram View** allows you to visually see how the data sources are connected and the different applied transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f3ad053-4d75-471c-a871-c0606e640e5e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (4) Data Preview pane\n",
    "**The Data Preview pane** only shows a subset of data to allow you to see which transformations you should make and how they affect the data. \n",
    "- You can also interact with the preview pane by dragging and dropping columns to change order or right-clicking on columns to filter or make changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "794d1d6b-8b34-4e82-bad7-f3d74af6542a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (5) Query Settings pane\n",
    "**The Query Settings pane** primarily includes **Applied Steps**. \n",
    "- Each transformation you do is tied to a step, some of which are automatically applied when you connect the data source. Depending on the complexity of the transformations, you may have several applied steps for each query.\n",
    "\n",
    "While this visual interface is helpful, you can also view the M code through **Advanced editor**.\n",
    "\n",
    "<img src=\"./images/03/power-query-advanced-editor.png\" alt=\"Power Query Advanced Editor\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "In the Query settings pane, you can see a **Data Destination** field where you can set the Lakehouse as your destination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efdb77b0-f151-413d-8a5c-3e83ab46890c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\">  If made available, data analysts can also connect to the dataflow through Power BI Desktop.\n",
    "\n",
    "![Power BI Desktop Get Data Connectors](./images/03/power-bi-desktop-dataflow-connectors.png)\n",
    "<img src=\"aaa\" alt=\"Description\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07c65631-4fbd-4db9-9408-59f17fdc64f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Integrate Dataflows Gen2 and Pipelines\n",
    "**Dataflows Gen2** provide an excellent option for data transformations in Microsoft Fabric. The combination of dataflows and pipelines is useful when you need to **perform additional operations** on the transformed data.\n",
    "\n",
    "**Data pipelines** are easily created in the Data Factory and Data Engineering workloads. Pipelines are a common concept in data engineering and offer a wide variety of activities to orchestrate. Some common activities include:\n",
    "- Copy data\n",
    "- Incorporate Dataflow\n",
    "- Add Notebook\n",
    "- Get metadata\n",
    "- Execute a script or stored procedure\n",
    "\n",
    "<img src=\"./images/03/pipelines-options.png\" alt=\"Pipeline Options\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bde7a422-5898-4245-bda0-70b134f4dd11",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Pipelines provide a visual way to complete activities in a specific order. \n",
    "- You can use a **`dataflow`** for data ingestion and transformation, and landing into a Lakehouse using dataflows. \n",
    "- Then incorporate the **`dataflow`** into a **`pipeline`** to orchestrate extra activities, like execute scripts or stored procedures after the dataflow has completed.\n",
    "\n",
    "<img src=\"./images/03/pipeline-dataflow-markup.png\" alt=\"Pipeline & Dataflow\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Ingest Data with Dataflows Gen2 in Microsoft Fabric",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
