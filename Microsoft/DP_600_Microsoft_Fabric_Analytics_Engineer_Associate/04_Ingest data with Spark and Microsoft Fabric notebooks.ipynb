{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05978be5-a1d2-4f88-8f07-9d4fed7bb564",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Connect to data with Spark\n",
    "Notebooks provide a comprehensive, automated solution for ingestion and transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9b4eb9d-82ef-4ce7-93c6-c9033f6a0f5f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Explore Fabric notebooks\n",
    "Fabric notebooks can be easily created in many parts of the Fabric service. \n",
    "- Notebooks are stored in the workspace they're created from, which may not be the same workspace where the lakehouse exists.\n",
    "\n",
    "![Notebook Overview](./images/04/2-notebook-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b29054f-fa47-40a6-919f-27c85a870235",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Connect to external sources\n",
    "Fabric Notebooks offer intuitive shortcuts for certain platforms. However, if your data resides elsewhere, you need another method. Here's a basic example of connecting to Azure blob storage with Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "794d9ac8-63f1-4a78-9817-d8eb531aeef2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Azure Blob Storage access info\n",
    "blob_account_name = \"azureopendatastorage\"\n",
    "blob_container_name = \"nyctlc\"\n",
    "blob_relative_path = \"yellow\"\n",
    "blob_sas_token = \"sv=2022-11-02&ss=bfqt&srt=c&sp=rwdlacupiytfx&se=2023-09-08T23:50:02Z&st=2023-09-08T15:50:02Z&spr=https&sig=abcdefg123456\" \n",
    "\n",
    "# Construct the path for connection\n",
    "wasbs_path = f'wasbs://{blob_container_name}@{blob_account_name}.blob.core.windows.net/{blob_relative_path}?{blob_sas_token}'\n",
    "\n",
    "# Read parquet data from Azure Blob Storage path\n",
    "blob_df = spark.read.parquet(wasbs_path)\n",
    "\n",
    "# Show the Azure Blob DataFrame\n",
    "blob_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f340a0a4-163e-4ff7-b456-00a632070fa7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Configure alternate authentication\n",
    "The previous example connects to the source and reads the data into a DataFrame. Depending on your source, you may need a different authentication type, such as Service Principal, OAuth, etc. Here's an example connecting to an Azure SQL Database with a Service Principal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "710da40e-96bc-4bcf-9237-342861a03c24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Placeholders for Azure SQL Database connection info\n",
    "server_name = \"your_server_name.database.windows.net\"\n",
    "port_number = 1433  # Default port number for SQL Server\n",
    "database_name = \"your_database_name\"\n",
    "table_name = \"YourTableName\" # Database table\n",
    "client_id = \"YOUR_CLIENT_ID\"  # Service principal client ID\n",
    "client_secret = \"YOUR_CLIENT_SECRET\"  # Service principal client secret\n",
    "tenant_id = \"YOUR_TENANT_ID\"  # Azure Active Directory tenant ID\n",
    "\n",
    "\n",
    "# Build the Azure SQL Database JDBC URL with Service Principal (Active Directory Integrated)\n",
    "jdbc_url = f\"jdbc:sqlserver://{server_name}:{port_number};database={database_name};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;Authentication=ActiveDirectoryIntegrated\"\n",
    "\n",
    "# Properties for the JDBC connection\n",
    "properties = {\n",
    "    \"user\": client_id, \n",
    "    \"password\": client_secret,  \n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "    \"tenantId\": tenant_id  \n",
    "}\n",
    "\n",
    "# Read entire table from Azure SQL Database using AAD Integrated authentication\n",
    "sql_df = spark.read.jdbc(url=jdbc_url, table=table_name, properties=properties)\n",
    "\n",
    "# Show the Azure SQL DataFrame\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06376f0e-09fd-4706-bfa1-078b2eda543f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Write data into a lakehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9aa52c6c-2bab-43a8-80e0-00cd3938c624",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Write to a file\n",
    "Lakehouses support structured, semi-structured, and unstructured files. Load as a **parquet file** or **Delta table** to take advantage of the Spark engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8119abcf-161c-4e33-ac6e-134578b19f77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write DataFrame to Parquet file format\n",
    "parquet_output_path = \"dbfs:/FileStore/your_folder/your_file_name\"\n",
    "df.write.mode(\"overwrite\").parquet(parquet_output_path)\n",
    "print(f\"DataFrame has been written to Parquet file: {parquet_output_path}\")\n",
    "\n",
    "# Write DataFrame to Delta table\n",
    "delta_table_name = \"your_delta_table_name\"\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(delta_table_name)\n",
    "print(f\"DataFrame has been written to Delta table: {delta_table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bcb77e8-caef-4a9e-b13c-2e72357dfca7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "OneLake supports a wide variety of file formats, including many formats that are commonly used in Spark code - such as delimited text, JSON, Parquet, Avro, ORC, and others. \n",
    "- In most cases, **`Parquet`** is the preferred format because of its optimized columnar storage structure and efficient compression capabilities. Parquet is also the base format on which Delta tables in a lakehouse are based.\n",
    "- **`Delta tables`** are one of the key features of Fabric lakehouses. Easily ingest and load your external data into a Delta table via notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4070352-1d41-43a9-a1b0-aa1aa3f50d69",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Optimize Delta table writes\n",
    "Fabric notebooks easily scale for large data, therefore read and write optimization is key. Consider these optimization functions for even more performant data ingestion.\n",
    "- **`V-Order`:** enables faster and more efficient reads by various compute engines, such as Power BI, SQL, and Spark.\n",
    "  - V-order applies special sorting, distribution, encoding, and compression on parquet files at write-time.\n",
    "- **`Optimize write`:** improves the performance and reliability by reducing the number of files written and increasing their size. \n",
    "  - It's useful for scenarios where the Delta tables have suboptimal or nonstandard file sizes, or where the extra write latency is tolerable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b61070cf-14c4-4071-bd05-cb432c44eb4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enable V-Order \n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "\n",
    "# Enable automatic Delta optimized write\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97a284eb-e706-4714-8bf5-21b268088766",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> Learn more about [Delta Lake table optimization and V-Order](https://learn.microsoft.com/en-us/fabric/data-engineering/delta-optimization-and-v-order?tabs=sparksql)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c5d8f69-65d5-4258-ac1b-4b9a793df0f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Consider uses for ingested data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3c96920-156f-4d03-b4c8-ab972182b1c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Transform for different users\n",
    "When you load data, it's a good idea to do some basic cleaning like removing duplicates, handling errors, converting null values, and getting rid of empty entries to ensure data quality and consistency. Also thinking about the different people who use the data is crucial.\n",
    "\n",
    "**Data scientists** usually prefer fewer changes so they can explore wide tables. They would likely want access to the raw ingested data. Fabric's Data Wrangler then lets them explore the data and generate transformation code for their specific needs.\n",
    "\n",
    "Whereas Power BI **data analysts** may require more transformation and modeling before they can use the data. While Power BI can transform data, starting with well-prepared data allows analysts to develop reports and insights more efficiently."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Ingest data with Spark and Microsoft Fabric notebooks",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
