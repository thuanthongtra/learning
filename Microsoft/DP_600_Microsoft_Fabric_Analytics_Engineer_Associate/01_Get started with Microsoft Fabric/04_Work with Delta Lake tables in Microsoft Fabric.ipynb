{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0b39fa2-cf21-4add-9aea-c389fa295f4e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Understand Delta Lake\n",
    "**Delta Lake:** is an open-source **storage layer** that adds relational database semantics to Spark-based data lake processing. \n",
    "- Tables in Microsoft Fabric lakehouses are **`Delta tables`**, which is signified by the triangular Delta (â–´) icon on tables in the lakehouse user interface.\n",
    "\n",
    "<img src=\"../images/01_Get started with Microsoft Fabric/04/delta-table.png\" alt=\"Delta Table\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "Delta tables are schema abstractions over data files that are stored in **Delta format**. \n",
    "- For each table, the lakehouse stores a folder containing **Parquet data files** and a **`_delta_Log`** folder in which transaction details are logged in **`JSON format`**.\n",
    "\n",
    "<img src=\"../images/01_Get started with Microsoft Fabric/04/delta-files.png\" alt=\"Parquet files\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb5eae65-b956-448b-8227-0d9bbe6c2f3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Benefits\n",
    "The benefits of using Delta tables include:\n",
    "\n",
    "- **Relational tables that support `querying` and data `modification`.** \n",
    "  - With Apache Spark, you can store data in Delta tables that support **CRUD (create, read, update, and delete)** operations. \n",
    "- **Support for `ACID` transactions.** \n",
    "  - Relational databases are designed to support transactional data modifications that provide:\n",
    "    - **`A`** tomicity (transactions complete as a single unit of work)\n",
    "    - **`C`** onsistency (transactions leave the database in a consistent state)\n",
    "    - **`I`** solation (in-process transactions can't interfere with one another)\n",
    "    - **`D`** urability (when a transaction completes, the changes it made are persisted). \n",
    "  - Delta Lake brings this same transactional support to Spark by implementing a transaction log and enforcing serializable isolation for concurrent operations.\n",
    "- **`Data versioning` and `time travel`.** \n",
    "  - Because all transactions are logged in the **`transaction log`**, you can **track multiple versions** of each table row and even use the time travel feature to **retrieve a previous version** of a row in a query.\n",
    "- **Support for `batch` and `streaming` data.** \n",
    "  - While most relational databases include tables that store static data, Spark includes native support for streaming data through the **Spark Structured Streaming API**. Delta Lake tables can be used as both sinks (destinations) and sources for streaming data.\n",
    "- **Standard formats and interoperability.** \n",
    "  - The underlying data for Delta tables is stored in Parquet format, which is commonly used in data lake ingestion pipelines. Additionally, you can use the **`SQL analytics endpoint`** for the Microsoft Fabric lakehouse to query Delta tables in SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59429378-f755-4a43-8968-d49acb1ee6df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Create delta tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d90cc936-3bc8-4fbd-868c-2885bd8c5403",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create managed vs external tables\n",
    "Creating both the **`table schema definition`** in the metastore and the **`data files`** in delta format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3922db9-669b-495f-8fd5-e96451e6bf76",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Managed tables\n",
    "- The table definition in the metastore and the underlying data files are **`both`** managed by the Spark runtime for the Fabric lakehouse. \n",
    "- Deleting the table will also **`delete` the underlying files** from the Tables storage location for the lakehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c241ac9-523f-431f-a647-0f71f58a2418",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load a file into a dataframe\n",
    "df = spark.read.load('Files/mydata.csv', format='csv', header=True)\n",
    "\n",
    "# Save the dataframe as a delta table\n",
    "df.write.format(\"delta\").saveAsTable(\"mytable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8a56c8d-0097-4227-99e7-ee8ada014124",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The code specifies that:\n",
    "- The table should be saved in **`delta format`** with a specified table name. \n",
    "- The data for the table is saved in **`Parquet files`** (regardless of the format of the source file you loaded into the dataframe) in the Tables storage area in the lakehouse, along with a **`_delta_log`** folder containing the transaction logs for the table. \n",
    "- The table will be listed in the Tables folder for the lakehouse in the Data explorer pane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc4c0e75-2ffa-4988-afb1-73cfd75c4112",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### External tables\n",
    "- The table definition in the metastore is **`mapped`** to an alternative file storage location.\n",
    "- Deleting an external table from the lakehouse metastore **`does not delete` the associated data files**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50aad691-e280-4cdf-b9dd-7f11af5c9734",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").saveAsTable(\"myexternaltable\", path=\"Files/myexternaltable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9e14613-4024-453b-9775-620793e37e41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# You can also specify a fully qualified path for a storage location, like this:\n",
    "df.write.format(\"delta\").saveAsTable(\"myexternaltable\", path=\"abfss://my_store_url..../myexternaltable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0d3973f-1ee4-4350-bfa6-575e4536086c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- The table definition is created in the **`metastore`** (so the table is listed in the **Tables** user interface for the lakehouse), but the Parquet data files and JSON log files for the table are stored in the **`Files storage location`** (and will be shown in the **Files node** in the **Lakehouse explorer pane**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a8ddfac-df77-44c9-ad66-785ffda58d3e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Creating table metadata\n",
    "Creates the **`table schema`** in the metastore **`without saving any data files`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "603931b8-1edb-4666-8430-f1a9b1dcc777",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Use the DeltaTableBuilder API\n",
    "The **`DeltaTableBuilder API`** enables you to write Spark code to create a table based on your specifications. \n",
    "\n",
    "For example, the following code creates a table with a specified name and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9176634b-2c03-43bb-8f35-1e7286bf5a2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "DeltaTable.create(spark) \\\n",
    "  .tableName(\"products\") \\\n",
    "  .addColumn(\"Productid\", \"INT\") \\\n",
    "  .addColumn(\"ProductName\", \"STRING\") \\\n",
    "  .addColumn(\"Category\", \"STRING\") \\\n",
    "  .addColumn(\"Price\", \"FLOAT\") \\\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "541ed947-836d-44bd-bfe8-3351306af17d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Use Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4351454d-8f27-472d-8559-39a3be626b9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Managed table\n",
    "CREATE TABLE salesorders\n",
    "(\n",
    "    Orderid INT NOT NULL,\n",
    "    OrderDate TIMESTAMP NOT NULL,\n",
    "    CustomerName STRING,\n",
    "    SalesTotal FLOAT NOT NULL\n",
    ")\n",
    "USING DELTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90365fb2-0bc3-4d54-ab9e-c1105671c1c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- External table: Specifying a LOCATION parameter\n",
    "CREATE TABLE MyExternalTable\n",
    "USING DELTA\n",
    "LOCATION 'Files/mydata'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0152ac10-dad1-44b5-896c-4669fb7aedcf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Saving data in delta format\n",
    "Save data in **`delta format without creating a table definition`** in the metastore\n",
    "\n",
    "- After saving the delta file, the path location you specified includes Parquet files containing the data and a **`_delta_log`** folder containing the transaction logs for the data. \n",
    "  - Any modifications made to the data through the delta lake API or in an external table that is subsequently created on the folder will be **recorded in the transaction logs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "984bb6ad-1c39-4e7a-9b05-81f45c420c45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PySpark code saves a dataframe to a new folder location in delta format:\n",
    "delta_path = \"Files/mydatatable\"\n",
    "df.write.format(\"delta\").save(delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "752060aa-ae81-4dcf-9eb5-a5bd3db1c9e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# using the overwrite mode\n",
    "new_df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "# using the append mode\n",
    "new_rows_df.write.format(\"delta\").mode(\"append\").save(delta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4ab192e-e0a7-48cd-9cc1-aae1a50e896c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> If you use the technique described here to save a dataframe to the Tables location in the lakehouse, Microsoft Fabric uses an automatic table discovery capability to create the corresponding table metadata in the metastore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "018eaa97-43e1-4273-ae52-07bc7a05ebd6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Work with delta tables in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b63461ad-cf0f-41d7-b2a9-f859e9cb44b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Using Spark SQL\n",
    "The most common way to work with data in delta tables in Spark is to use **Spark SQL**. \n",
    "- You can embed SQL statements in other languages (such as PySpark or Scala) by using the **`spark.sql`** library. \n",
    "\n",
    "For example, the following code inserts a row into the products table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75f3f313-c4a4-41d7-9113-7a7ca6b33c77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"INSERT INTO products VALUES (1, 'Widget', 'Accessories', 2.99)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e829bf9-42a7-42ee-80e1-09f02633fe20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "UPDATE products\n",
    "SET Price = 2.49 WHERE ProductId = 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fdd61dc-6f65-4a34-ba66-96866ac8949a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Use the Delta API\n",
    "When you want to work with **delta files** rather than **catalog tables**, it may be simpler to use the **`Delta Lake API`**. \n",
    "- You can create an **instance of a DeltaTable** from a folder location containing files in delta format, and then use the API to modify the data in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9da645f3-99ba-4d6a-9d1e-8fd4ead29d04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a DeltaTable object\n",
    "delta_path = \"Files/mytable\"\n",
    "deltaTable = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# Update the table (reduce price of accessories by 10%)\n",
    "deltaTable.update(\n",
    "    condition = \"Category == 'Accessories'\",\n",
    "    set = { \"Price\": \"Price * 0.9\" })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c208865-4a1d-484e-9316-cb35f03322a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Use time travel to work with table versioning\n",
    "Modifications made to delta tables are logged in the transaction log for the table. \n",
    "- You can use the logged transactions to view the **`history of changes`** made to the table and to retrieve older versions of the data (known as time travel)\n",
    "\n",
    "To see the history of a table, you can use the **`DESCRIBE SQL`** command as shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a8e41b4-6ffd-4782-a163-c8db0c8814a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Managed table\n",
    "DESCRIBE HISTORY products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72be5d17-9ac8-4d6e-9ab2-ff5f263dd2a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- External table\n",
    "DESCRIBE HISTORY 'Files/mytable'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91775f2a-b24c-48d4-bd3c-05baa5267e59",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"../images/01_Get started with Microsoft Fabric/04/time_travel.png\" alt=\"Time Travel\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46ff3537-199e-4092-abed-69855cf28e7c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Retrieve historical information\n",
    "You can retrieve data from a **specific version** of the data by reading the delta file location into a dataframe, specifying the version required as a **`versionAsOf`** option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f436c46-c01b-4949-b196-c4aecee473c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe950fd1-7d8a-485f-9c9a-3bcff1bd8d52",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Or specify a timestamp by using the **`timestampAsOf`** option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa2bcfb4-3681-43cf-a1e3-23dbddf25a4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").option(\"timestampAsOf\", '2022-01-01').load(delta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "506962f4-f2c8-4721-9293-db0747fca200",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Use delta tables with streaming data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffa19f2f-35dd-4286-b4a8-294fab034da8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark Structured Streaming\n",
    "Spark includes **`native support`** for streaming data through **Spark Structured Streaming**, an API that is based on a **boundless dataframe** in which streaming data is captured for processing. \n",
    "- A **Spark Structured Streaming** dataframe can read data from many different kinds of streaming source, including network ports, real time message brokering services such as Azure Event Hubs or Kafka, or file system locations.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> For more information about Spark Structured Streaming, see [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) in the Spark documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e507f27-317e-4193-bc04-7f3d43dfb6fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Streaming with delta tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a306fd14-7b0d-47bd-ae98-86c69b1f5744",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Using a delta table as a streaming source\n",
    "In the following PySpark example, a delta table is used to store details of Internet sales orders. A stream is created that reads data from the table folder as new data is appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8733b845-687c-4361-a894-585a8b9e7ace",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Load a streaming dataframe from the Delta Table\n",
    "stream_df = spark.readStream.format(\"delta\") \\\n",
    "    .option(\"ignoreChanges\", \"true\") \\\n",
    "    .load(\"Files/delta/internetorders\")\n",
    "\n",
    "# Now you can process the streaming data in the dataframe\n",
    "# for example, show it:\n",
    "stream_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d74b872b-d46c-4b41-aaf2-e59e7910af08",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "After reading the data from the delta table into a streaming dataframe, you can use the Spark Structured Streaming API to **process** it. \n",
    "- In the example above, the dataframe is simply displayed; but you could use Spark Structured Streaming to **aggregate** the data over temporal windows (for example to count the number of orders placed every minute) and send the aggregated results to a downstream process for near-real-time visualization.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> When using a delta table as a streaming source, only append operations can be included in the stream. Data modifications will cause an error unless you specify the **`ignoreChanges`** or **`ignoreDeletes`** option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61e01467-a775-49c8-aa6f-eed9015ea39d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Using a delta table as a streaming sink\n",
    "In the following PySpark example, a stream of data is read from JSON files in a folder. The JSON data in each file contains the status for an IoT device in the format **`{\"device\":\"Dev1\",\"status\":\"ok\"}`** \n",
    "- New data is added to the stream whenever a file is added to the folder. \n",
    "- The input stream is a boundless dataframe, which is then written in **delta format** to a folder location for a delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "865a1995-e3cb-4f6a-8559-4aaf306f3a67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a stream that reads JSON data from a folder\n",
    "inputPath = 'Files/streamingdata/'\n",
    "jsonSchema = StructType([\n",
    "    StructField(\"device\", StringType(), False),\n",
    "    StructField(\"status\", StringType(), False)\n",
    "])\n",
    "stream_df = spark.readStream.schema(jsonSchema).option(\"maxFilesPerTrigger\", 1).json(inputPath)\n",
    "\n",
    "# Write the stream to a delta table\n",
    "table_path = 'Files/delta/devicetable'\n",
    "checkpoint_path = 'Files/delta/checkpoint'\n",
    "delta_stream = stream_df.writeStream.format(\"delta\").option(\"checkpointLocation\", checkpoint_path).start(table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f73eb096-6fcc-4261-b64a-8a2a7ce2a8f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> The **`checkpointLocation`** option is used to write a checkpoint file that **tracks the state** of the stream processing. This file enables you to recover from failure at the point where stream processing left off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c05872f-caee-4e2d-8c1c-e1a03f3544bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "After the streaming process has started, you can query the Delta Lake table to which the streaming output is being written to see the latest data. \n",
    "\n",
    "For example, the following code creates a catalog table for the Delta Lake table folder and queries it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab7d0138-ff09-4427-9228-89aa7a10b108",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE DeviceTable\n",
    "USING DELTA\n",
    "LOCATION 'Files/delta/devicetable';\n",
    "\n",
    "SELECT device, status\n",
    "FROM DeviceTable;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64449b1a-8821-49cb-8ad0-88267a0ce163",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To stop the stream of data being written to the Delta Lake table, you can use the stop method of the streaming query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39a06528-2f4f-4ef3-968a-7039c077e51f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delta_stream.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6a6a94c-11ff-4224-aa19-05dfb19acbaf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> For more information about using delta tables for streaming data, see [Table streaming reads and writes](https://docs.delta.io/latest/delta-streaming.html) in the Delta Lake documentation."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Work with Delta Lake tables in Microsoft Fabric",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
