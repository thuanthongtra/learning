{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05978be5-a1d2-4f88-8f07-9d4fed7bb564",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Explore the Microsoft Fabric Lakehouse\n",
    "**A Lakehouse** presents as a database and is built on top of a **data lake** using **`Delta` format tables**.\n",
    "- Lakehouses combine the SQL-based analytical capabilities of a **`relational data warehouse`** and the flexibility and scalability of a **`data lake`**. \n",
    "- Lakehouses store all data formats and can be used with various analytics tools and programming languages.\n",
    "\n",
    "<img src=\"../images/01_Get started with Microsoft Fabric/02/lakehouse-components.png\" alt=\"Lakehouse Components\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e7847c3-716c-4ec4-8fd3-c047298720fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Benefits\n",
    "- Lakehouses use **`Spark`** and **`SQL engines`** to process large-scale data and support ML or predictive modeling analytics.\n",
    "- Lakehouse data is organized in a **`schema-on-read`** format, which means you define the schema as needed rather than having a predefined schema.\n",
    "- Lakehouses support **`ACID`** (Atomicity, Consistency, Isolation, Durability) **transactions** through **Delta Lake formatted tables** for data consistency and integrity.\n",
    "- Lakehouses are a **`single location`** for data engineers, data scientists, and data analysts to access and use data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a325c4d4-1822-4976-bb64-fa22d8e978d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Microsoft Fabric lakehouses\n",
    "In Microsoft Fabric, you can \n",
    "1. Create a lakehouse in any **premium tier workspace**. \n",
    "2. Load data - in any common format - from various sources; including local files, databases, or APIs. \n",
    "    - Data ingestion can also be automated using **Data Factory Pipelines** or **Dataflows (Gen2)** in Microsoft Fabric.\n",
    "      - **Data Factory Pipelines** can be used to orchestrate Spark, Dataflow, and other activities; enabling you to implement complex data transformation processes.\n",
    "      - **Dataflows (Gen2)** are based on **Power Query** - a familiar tool to data analysts using Excel or Power BI that provides visual representation of transformations as an alternative to traditional programming.\n",
    "3. Create Fabric **shortcuts** to data in external sources, such as **Azure Data Lake Store Gen2** or a **Microsoft OneLake** location outside of the lakehouse's own storage. \n",
    "    - The **Lakehouse Explorer** enables you to browse files, folders, shortcuts, and tables; and view their contents within the Fabric platform.\n",
    "4. Use **Notebooks** or **Dataflows (Gen2)** to explore and transform it.\n",
    "5. Query it using SQL, use it to train machine learning models, perform real-time intelligence, or develop reports in Power BI.\n",
    "6. Apply **data governance policies** to your Lakehouse, such as data classification and access control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a47210d-5939-4f59-86c7-1326f1abf1f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Work with Microsoft Fabric Lakehouses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d818c88-9093-4148-97d8-4a714d9bf1fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create and explore a lakehouse\n",
    "You create and configure a new Lakehouse in the Data Engineering workload. Each lakehouse produces **three named items** in the Fabric-enabled workspace:\n",
    "- **Lakehouse:** is the lakehouse **`storage` and `metadata`**, where you interact with files, folders, and table data.\n",
    "- **Semantic model (default):** is an **automatically created `semantic model`** based on the tables in the lakehouse.\n",
    "  - **Power BI reports** can be built from the semantic model.\n",
    "- **SQL analytics endpoint:** is a **read-only** SQL analytics endpoint through which you can **_connect_** and **_query_** data with **Transact-SQL**.\n",
    "\n",
    "<img src=\"../images/01_Get started with Microsoft Fabric/02/lakehouse-items.png\" alt=\"Three Lakehouse items\" style=\"border: 2px solid black; border-radius: 10px;\">\n",
    "\n",
    "You can work with the data in the lakehouse in two modes:\n",
    "- **Lakehouse:** enables you to **_add_** and **_interact_** with tables, files, and folders in the Lakehouse.\n",
    "- **SQL analytics endpoint:** enables you to **use SQL to _query_** the tables in the lakehouse and manage its relational semantic model.\n",
    "\n",
    "<img src=\"../images/01_Get started with Microsoft Fabric/02/explorer-modes.png\" alt=\"Two Lakehouse Explorer modes\" style=\"border: 2px solid black; border-radius: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "883dc6a3-ae87-4074-8956-079f465af40b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Ingest data into a lakehouse\n",
    "There are many ways to load data into a Fabric lakehouse, including:\n",
    "- **Upload:** Upload **local files or folders** to the lakehouse. You can then explore and process the file data, and load the results into tables.\n",
    "- **Dataflows (Gen2):** Import and transform data from **multiple sources** using **`Power Query Online`**, and load it directly into a table.\n",
    "- **Notebooks:** Use notebooks in Fabric to ingest and transform data, and load it into tables or files.\n",
    "- **Data Factory pipelines:** Copy data and orchestrate data processing **activities**, loading the results into tables or files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "753b43f5-2302-4a9b-a80f-beb66b781bf0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Access data using shortcuts\n",
    "**Shortcuts:** enable you to **_integrate_** data into your lakehouse while keeping it stored in external storage.\n",
    "- Shortcuts are useful when you need to source data that's in a **`different storage account`** or even a **`different cloud provider`**. \n",
    "  - Within your Lakehouse you can create shortcuts that point to **_different storage accounts_** and other Fabric items like data warehouses, KQL databases, and other Lakehouses.\n",
    "- Shortcuts can be created in both **Lakehouses** and **KQL databases**, and appear as a folder in the lake. This allows Spark, SQL, Real-Time intelligence and Analysis Services to all utilize shortcuts when querying data.\n",
    "\n",
    "Source data **permissions and credentials** are all managed by **`OneLake`**. \n",
    "- When accessing data through a shortcut to another OneLake location, the **identity** of the calling user will be utilized to **_authorize_** **access** to the data in the target path of the shortcut.\n",
    "- The user **_must have permissions_** in the target location to read the data.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\" alt=\"Note\"> For more information on how to use shortcuts, see [OneLake shortcuts documentation](https://learn.microsoft.com/en-us/fabric/onelake/onelake-shortcuts) in the Microsoft Fabric documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bdc808d-de0d-4c35-b6d6-6d296dbbf036",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Explore and transform data in a lakehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3310499d-ebac-4617-ab2b-c57eeb2492f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Explore and Transform\n",
    "After loading data into the lakehouse, you can use various tools and techniques to explore and transform it, including:\n",
    "- **Apache Spark:** Each Fabric lakehouse can use **Spark pools** through **`Notebooks`** or **`Spark Job Definitions`** to process data in files and tables in the lakehouse using Scala, PySpark, or Spark SQL.\n",
    "  - **Notebooks:** **Interactive coding interfaces** in which you can use code to read, transform, and write data directly to the lakehouse as tables and/or files.\n",
    "  - **Spark job definitions:** On-demand or scheduled scripts that use the **Spark engine to process data** in the lakehouse.\n",
    "- **SQL analytic endpoint:** To run **Transact-SQL** statements to query, filter, aggregate, and otherwise explore data in lakehouse tables.\n",
    "- **Dataflows (Gen2):** In addition to using a dataflow to ingest data into the lakehouse, you can **_create_ a dataflow** to perform subsequent transformations through **Power Query**, and optionally land transformed data back to the Lakehouse.\n",
    "- **Data pipelines:** Orchestrate **complex data transformation logic** that operates on data in the lakehouse through a sequence of **activities** (such as dataflows, Spark jobs, and other control flow logic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b25bf0c-5399-4b71-948a-ae09be8ae458",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Analyze and Visualize\n",
    "The data in your lakehouse tables is included in a **`semantic model`** that defines a **`relational model`** for your data.\n",
    "- You can edit this semantic model (or create other semantic models), defining custom measures, hierarchies, aggregations, and other elements of a semantic model. \n",
    "- You can then use the semantic model as the **source for a `Power BI report`** that enables you to visualize and analyze the data.\n",
    "\n",
    "By combining the data visualization capabilities of Power BI with the centralized storage and tabular schema of a data lakehouse, you can implement an end-to-end analytics solution on a single platform."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Get started with lakehouses in Microsoft Fabric",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
